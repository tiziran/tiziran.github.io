@inproceedings{Lowe1999,
abstract = {An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds},
archivePrefix = {arXiv},
arxivId = {cs/0112017},
author = {Lowe, David G.},
booktitle = {Proceedings of the Seventh IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.1999.790410},
eprint = {0112017},
isbn = {0-7695-0164-8},
issn = {0-7695-0164-8},
number = {[8},
organization = {IEEE},
pages = {1150--1157},
pmid = {15806121},
primaryClass = {cs},
title = {{Object recognition from local scale-invariant features}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=790410},
volume = {2},
year = {1999}
}

@inproceedings{Montemerlo2002,
abstract = {The ability to simultaneously localize a robot and accurately map its surroundings is considered by many to be a key prerequisite of truly autonomous robots. However, few approaches to this problem scale up to handle the very large number of landmarks present in real environments. Kalman filter-based algorithms, for example, require time quadratic in the number of landmarks to incorporate each sensor observation. This paper presents FastSLAM, an algorithm that recursively estimates the full posterior distribution over robot pose and landmark locations, yet scales logarithmically with the number of landmarks in the map. This algorithm is based on a factorization of the posterior into a product of conditional landmark distributions and a distribution over robot paths. The algorithm has been run successfully on as many as 50,000 landmarks, environments far beyond the reach of previous approaches. Experimental results demonstrate the advantages and limitations of the FastSLAM algorithm on both simulated and real-world data.},
author = {Montemerlo, Michael and Thrun, Sebastian and Koller, Daphne and Wegbreit, Ben},
booktitle = {Proc. of 8th National Conference on Artificial Intelligence/14th Conference on Innovative Applications of Artificial Intelligence},
doi = {10.1.1.16.2153},
isbn = {0262511290},
number = {2},
pages = {593--598},
title = {{FastSLAM: A factored solution to the simultaneous localization and mapping problem}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:FastSLAM+:+A+Factored+Solution+to+the+Simultaneous+Localization+and+Mapping+Problem{\#}0},
volume = {68},
year = {2002}
}

@Book{Ilboudo2016,
  author        = {Ilboudo, Sidb??wendin David Olivier and Sombi??, Issa and Soubeiga, Andr?? Kamba and Dr??bel, Tania},
  publisher     = {Cambridge University Press, ISBN: 0521540518},
  title         = {{Facteurs influen??ant le refus de consulter au centre de sant?? dans la r??gion rurale Ouest du Burkina Faso}},
  year          = {2016},
  edition       = {Second},
  isbn          = {9788578110796},
  number        = {3},
  volume        = {28},
  abstract      = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1011.1669v3},
  booktitle     = {Sante Publique},
  doi           = {10.1017/CBO9781107415324.004},
  eprint        = {arXiv:1011.1669v3},
  issn          = {09953914},
  keywords      = {Burkina Faso, Delivery of health care, Motivation, Patient preference, Rural health services, Treatment refusal},
  pages         = {391--397},
  pmid          = {25246403},
  url           = {http://www.robots.ox.ac.uk/{~}vgg/hzbook/index.html},
}

@article{Pinto2008,
abstract = {Progress in understanding the brain mechanisms underlying vision requires the construction of computational models that not only emulate the brain's anatomy and physiology, but ultimately match its performance on visual tasks. In recent years, "natural" images have become popular in the study of vision and have been used to show apparently impressive progress in building such models. Here, we challenge the use of uncontrolled "natural" images in guiding that progress. In particular, we show that a simple V1-like model--a neuroscientist's "null" model, which should perform poorly at real-world visual object recognition tasks--outperforms state-of-the-art object recognition systems (biologically inspired and otherwise) on a standard, ostensibly natural image recognition test. As a counterpoint, we designed a "simpler" recognition test to better span the real-world variation in object pose, position, and scale, and we show that this test correctly exposes the inadequacy of the V1-like model. Taken together, these results demonstrate that tests based on uncontrolled natural images can be seriously misleading, potentially guiding progress in the wrong direction. Instead, we reexamine what it means for images to be natural and argue for a renewed focus on the core problem of object recognition--real-world image variation.},
archivePrefix = {arXiv},
arxivId = {arXiv:1202.2745v1},
author = {Pinto, Nicolas and Cox, David D. and DiCarlo, James J.},
doi = {10.1371/journal.pcbi.0040027},
eprint = {arXiv:1202.2745v1},
isbn = {1553-7358},
issn = {1553734X},
journal = {PLoS Computational Biology},
number = {1},
pages = {0151--0156},
pmid = {18225950},
publisher = {Public Library of Science},
title = {{Why is real-world visual object recognition hard?}},
url = {http://dx.doi.org/10.1371{\%}2Fjournal.pcbi.0040027},
volume = {4},
year = {2008}
}

@misc{Koller2009,
author = {Koller, D and Friedman, N},
booktitle = {Citeseer},
publisher = {MIT press},
title = {{Probabilistic graphical models: principles and techniques}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.404.3622{\&}rep=rep1{\&}type=pdf{\%}5Cnhttp://books.google.com/books?hl=en{\&}lr={\&}id=7dzpHCHzNQ4C{\&}oi=fnd{\&}pg=PR9{\&}dq=Probabilistic+graphical+models+principles+and+techniques{\&}ots=pu7FAm4{\_}tQ{\&}sig=i3UYsiOk3mjRASsvTV78a8gPeP8},
year = {2009}
}

@InProceedings{Abdullah2010a,
  author    = {Abdullah, Siti Norul Huda Sheikh and PirahanSiah, Farshid and {Zainal Abidin}, Nur Hanisah and Sahran, Shahnorbanun},
  booktitle = {International Conference on Signal and Image Processing WASET Singapore August 25-27, 2010 ICSIP 2010},
  title     = {{Multi-threshold approach for license plate recognition system}},
  year      = {2010},
  pages     = {1046--1050},
  abstract  = {The objective of this paper is to propose an adaptive multi threshold for image segmentation precisely in object detection. Due to the different types of license plates being used, the requirement of an automatic LPR is rather different for each country. The proposed technique is applied on Malaysian LPR application. It is based on Multi Layer Perceptron trained by back propagation. The proposed adaptive threshold is introduced to find the optimum threshold values. The technique relies on the peak value from the graph of the number object versus specific range of threshold values. That proposed approach has actually increased the overall performance compared to current optimal threshold techniques. Further improvement on this method is in progress to accommodate real time system specification.},
  file      = {:farshid/conference/Multi-threshold approach for license plate_Singapore_2010.pdf:PDF},
  groups    = {Farshid PirahanSiah},
  priority  = {prio3},
  ranking   = {rank5},
  url       = {http://www.waset.org/journals/waset/v72/v72-146.pdf},
}

@InProceedings{Abdullah2010,
  author    = {Abdullah, Siti Norul Huda Sheikh and PirahanSiah, Farshid and Khalid, Marzuki and Omar, Khairuddin},
  booktitle = {2nd Malaysian Joint Conference on Artificial Intelligence (MJCAI 2010)},
  title     = {{An evaluation of classification techniques using enhanced Geometrical Topological Feature Analysis}},
  year      = {2010},
  pages     = {12--22},
  abstract  = {In this paper, we evaluate the best classification techniques for Malaysia license plate recognition (LPR)system. We also discuss four image classification techniques that are used in contemporary LPR sys- tem worldwide. There are artificial immune recognition system, neural network, bayesian network and support vector machine. We propose and apply enhanced geometrical topological feature analysis on Malaysian character and number images as their inputs. We also explain character error analysis based on those image classification approaches. It shows that support vector machine outperforms compared to other classifiers.},
  file      = {:farshid/conference/An evaluation of classification techniques using_Malaysia_2010.pdf:PDF},
  groups    = {Farshid PirahanSiah},
  keywords  = {Farshid PirahanSiah, Computer Science, threshold},
}

@Article{Bengio2013,
  author        = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  title         = {{Representation Learning: A Review and New Perspectives.}},
  journal       = {Tpami},
  year          = {2013},
  volume        = {35},
  number        = {1993},
  pages         = {1--30},
  issn          = {1939-3539},
  abstract      = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1206.5538v2},
  doi           = {3C2DBCEE-8A96-493B-B88B-36B1F52ECA58},
  eprint        = {arXiv:1206.5538v2},
  isbn          = {0162-8828 VO - 35},
  keywords      = {cs.LG},
  pmid          = {23459267},
  url           = {http://arxiv.org/abs/1206.5538{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/23459267},
}

@Article{Pirahansiah2013,
  author    = {Pirahansiah, Farshid and Norul, Siti and Sheikh, Huda and Sahran, Shahnorbanun},
  journal   = {Asia-Pacific Journal of Information Technology and Multimedia},
  title     = {{Simultaneous Localization and Mapping Trends and Humanoid Robot Linkages}},
  year      = {2013},
  number    = {2},
  pages     = {27--38},
  volume    = {2},
  file      = {:farshid/Journals/Simultaneous Localization And Mapping Trends And Humanoid Robot Linkages_2013.pdf:PDF},
  groups    = {Farshid PirahanSiah},
  keywords  = {3d vision, and mapping, cml, concurrent mapping and, humanoid robot, localisation, localization, stereo vision on slam, visual simultaneous localization, vslam},
  publisher = {Penerbit Universiti Kebangsaan Malaysia},
}

@Article{PirahanSiah2014,
  author    = {PirahanSiah, Farshid and Abdullah, Siti Norul Huda Sheikh and Sahran, Shahnorbanun},
  journal   = {Research Journal of Applied Sciences, Engineering and Technology},
  title     = {{Adaptive image thresholding based on the peak signal-to-noise Ratio}},
  year      = {2014},
  issn      = {20407467},
  number    = {9},
  pages     = {1104--1116},
  volume    = {8},
  file      = {:farshid/Journals/Adaptive Image Thresholding Based on the Peak Signal-to-noise Ratio_2014.pdf:PDF},
  groups    = {Farshid PirahanSiah},
  keywords  = {Image processing, Image segmentation, Optical character recognition, Single thresholding},
  publisher = {{\{}$\backslash$copyright{\}} Maxwell Scientific Organization},
}

@article{Ashtari2015,
abstract = {This paper proposes a fast algorithm for rotating images while preserving their quality. The new approach rotates images based on vertical or horizontal lines in the original image and their rotated equation in the target image. The proposed method is a one-pass method that determines a based-line equation in the target image and extracts all corresponding pixels on the base-line. Floating-point multiplications are performed to calculate the base-line in the target image, and other line coordinates are calculated using integer addition or subtraction and logical justifications from the base-line pixel coordinates in the target image. To avoid a heterogeneous distance between rotated pixels in the target image, each line rotates to two adjacent lines. The proposed method yields good performance in terms of speed and quality according to the results of an analysis of the computation speed and accuracy.},
author = {Ashtari, Amir Hossein and Nordin, Md Jan and Kahaki, Seyed Mostafa Mousavi},
doi = {10.1109/TIP.2015.2440763},
issn = {1057-7149},
journal = {IEEE Transactions on Image Processing},
keywords = {Accuracy;Computer vision;Image processing;Interpolation;Mathematical model;Shearing;Transforms;DLR;Double-line rotation;image representation;image rotation;image transform;line rotation},
month = {nov},
number = {11},
pages = {3370--3385},
title = {{Double Line Image Rotation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7117407},
volume = {24},
year = {2015}
}

@InProceedings{Szegedy2015,
  author        = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  title         = {{Going deeper with convolutions}},
  booktitle     = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  year          = {2015},
  volume        = {07-12-June-2015},
  pages         = {1--9},
  abstract      = {Abstract We propose a deep convolutional neural network architecture codenamed Incep- tion, which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  archiveprefix = {arXiv},
  arxivid       = {1409.4842},
  doi           = {10.1109/CVPR.2015.7298594},
  eprint        = {1409.4842},
  isbn          = {9781467369640},
  issn          = {10636919},
  pmid          = {24920543},
}

@Article{Russakovsky2015,
  author        = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
  journal       = {International Journal of Computer Vision},
  title         = {{ImageNet Large Scale Visual Recognition Challenge}},
  year          = {2015},
  issn          = {15731405},
  number        = {3},
  pages         = {211--252},
  volume        = {115},
  abstract      = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide detailed a analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
  archiveprefix = {arXiv},
  arxivid       = {1409.0575},
  doi           = {10.1007/s11263-015-0816-y},
  eprint        = {1409.0575},
  isbn          = {0920-5691},
  keywords      = {Benchmark, Dataset, Large-scale, Object detection, Object recognition},
  pmid          = {16190471},
}

@Article{Ghesu2016,
  author   = {Ghesu, Florin C. and Krubasik, Edward and Georgescu, Bogdan and Singh, Vivek and Zheng, Yefeng and Hornegger, Joachim and Comaniciu, Dorin},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{Marginal Space Deep Learning: Efficient Architecture for Volumetric Image Parsing}},
  year     = {2016},
  issn     = {1558254X},
  month    = {may},
  number   = {5},
  pages    = {1217--1228},
  volume   = {35},
  abstract = {Conference proceddings},
  doi      = {10.1109/TMI.2016.2538802},
  isbn     = {978-3-319-24552-2},
  keywords = {Deep learning, image parsing, marginal space learning, sparse representations, three-dimensional (3D) object detection and segmentation},
  pmid     = {23285570},
}

@Article{PirahanSiah2016,
  author    = {Farshid PirahanSiah and Mohammad Shahverdy},
  journal   = {International Journal of Computer Science and Network Solutions},
  title     = {{GSFT-PSNR: Global Single Fuzzy Threshold Based on PSNR for OCR Systems}},
  year      = {2016},
  number    = {6},
  pages     = {1--19},
  volume    = {4},
  file      = {:farshid/Journals/GSFT-PSNR Global Single Fuzzy Threshold_2016.pdf:PDF},
  groups    = {Farshid PirahanSiah},
  publisher = {International journal of Computer Science and Network Solutions (IJCSNS)},
}

@article{Jain2000,
abstract = {The primary goal of pattern recognition is supervised or unsupervised classification. Among the various frameworks in which pattern recognition has been traditionally formulated, the statistical approach has been most intensively studied and used in practice. More recently, neural network techniques and methods imported from statistical learning theory have been receiving increasing attention. The design of a recognition system requires careful attention to the following issues: definition of pattern classes, sensing environment, pattern representation, feature extraction and selection, cluster analysis, classifier design and learning, selection of training and test samples, and performance evaluation. In spite of almost 50 years of research and development in this field, the general problem of recognizing complex patterns with arbitrary orientation, location, and scale remains unsolved. New and emerging applications, such as data mining, web searching, retrieval of multimedia data, face recognition, and cursive handwriting recognition, require robust and efficient pattern recognition techniques. The objective of this review paper is to summarize and compare some of the well-known methods used in various stages of a pattern recognition system and identify research topics and applications which are at the forefront of this exciting and challenging field.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Jain, Anil K. and Duin, Robert P W and Mao, Jianchang},
doi = {10.1109/34.824819},
eprint = {arXiv:1011.1669v3},
isbn = {0162-8828 VO - 22},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {1},
pages = {4--37},
pmid = {17542025},
title = {{Statistical pattern recognition: A review}},
volume = {22},
year = {2000}
}

@Article{Perona1990,
  author   = {Perona, Pietro and Malik, Jitendra},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {{Scale-Space and Edge Detection Using Anisotropic Diffusion}},
  year     = {1990},
  issn     = {01628828},
  number   = {7},
  pages    = {629--639},
  volume   = {12},
  abstract = {A new definition of scale-space is suggested, and a class of algorithms used to realize a diffusion process is introduced. The diffusion coefficient is chosen to vary spatially in such a way as to encourage intraregion smoothing rather than interregion smoothing. It is shown that the 'no new maxima should be generated at coarse scales' property of conventional scale space is preserved. As the region boundaries in our approach remain sharp, a high-quality edge detector which successfully exploits global information is obtained. Experimental results are shown on a number of images. Parallel hardware implementations are made feasible because the algorithm involves elementary, local operations replicated over the image.},
  doi      = {10.1109/34.56205},
  file     = {:D$\backslash$:/Users/farshid.pirahansiah/Dropbox/1396/myPaper/reference/Perona1990 - Scale-space and edge detection using anisotropic diffusion.pdf:pdf},
  isbn     = {0162-8828},
  keywords = {Adaptive filtering, Analog VLSI, Edge detection, Edge enhancement, Nonlinear diffusion, Nonlinear filtering, Parallel algorithm, Scale-space},
  pmid     = {19268610},
  url      = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=56205},
}

@article{Scharstein2002,
abstract = {Stereo matching is one of the most active research areas in computer vision. While a large number of algorithms for stereo correspondence have been developed, relatively little work has been done on characterizing their performance. In this paper, we present a taxonomy of dense, two-frame stereo methods. Our taxonomy is designed to assess the different components and design decisions made in individual stereo algorithms. Using this taxonomy, we compare existing stereo methods and present experiments evaluating the performance of many different variants. In order to establish a common software platform and a collection of data sets for easy evaluation, we have designed a stand-alone, flexible C++ implementation that enables the evaluation of individual components and that can easily be extended to include new algorithms. We have also produced several new multi-frame stereo data sets with ground truth and are making both the code and data sets available on the Web. Finally, we include a comparative evaluation of a large set of today's best-performing stereo algorithms.},
author = {Scharstein, Daniel and Szeliski, Richard},
doi = {10.1023/A:1014573219977},
file = {:D$\backslash$:/Users/farshid.pirahansiah/Dropbox/1396/myPaper/reference/Scharstein2002 - A taxonomy and evaluation of dense two-frame stereo correspondence algorithms.pdf:pdf},
isbn = {0-7695-1327-1},
issn = {1573-1405},
journal = {International Journal of Computer Vision},
number = {1},
pages = {7--42},
pmid = {350},
title = {{A Taxonomy and Evaluation of Dense Two-Frame Stereo Correspondence Algorithms}},
url = {http://dx.doi.org/10.1023/A:1014573219977},
volume = {47},
year = {2002}
}

@inproceedings{Oertel2008,
abstract = {After nearly half a century of computer vision research, application-specific systems are common but the goal of developing a robust, general-purpose computer vision system remains out of reach. Rather than focus on the strengths and weaknesses of current computer vision approaches, this paper will enumerate and investigate the challenges that must be overcome before this goal can be achieved. Key challenges include handling variations in environment or acquisition parameters such as lighting, view angle, distance, and image quality; recognizing naturally occurring as well as intentionally deceptive variations in object appearance; providing robust general-purpose image segmentation and co-registration; generating 3D representations from 2D images; developing useful object representations; providing required knowledge that is not represented in the image itself; and managing computational complexity. Each of these challenges, along with their relevance to solving the vision problem, will be discussed. Understanding these challenges as a whole may provide insight into underlying mechanisms that will provide the backbone of a robust general-purpose computer vision system.},
address = {Washington, DC, USA},
author = {Oertel, Carsten and Colder, Brian and Colombe, Jeffrey and High, Julia and Ingram, Michael and Sallee, Phil},
booktitle = {Proceedings - Applied Imagery Pattern Recognition Workshop},
doi = {10.1109/AIPR.2008.4906457},
isbn = {9781424431250},
issn = {15505219},
pages = {1--8},
publisher = {IEEE Computer Society},
title = {{Current challenges in automating visual perception}},
url = {http://dx.doi.org/10.1109/AIPR.2008.4906457},
year = {2008}
}

@InProceedings{Elgammal2000,
  author    = {Elgammal, Ahmed and Harwood, David and Davis, Larry},
  booktitle = {Computer Vision—ECCV 2000},
  title     = {{Non-parametric model for background subtraction}},
  year      = {2000},
  pages     = {751--767},
  volume    = {1843},
  abstract  = {Background subtraction is a method typically used to segment moving regions in image sequences taken from a static camera by comparing each new frame to a model of the scene background. We present a novel non-parametric background model and a background subtraction approach. The model can handle situations where the background of the scene is cluttered and not completely static but contains small motions such as tree branches and bushes. The model estimates the probability of observing pixel intensity values based on a sample of intensity values for each pixel. The model adapts quickly to changes in the scene which enables very sensitive detection of moving targets. We also show how the model can use color information to suppress detection of shadows. The implementation of the model runs in real-time for both gray level and color imagery. Evaluation shows that this approach achieves very sensitive detection with very low false alarm rates.},
  doi       = {10.1007/3-540-45053},
  file      = {:D$\backslash$:/Users/farshid.pirahansiah/Dropbox/1396/myPaper/reference/Elgammal2000 - Non-parametric model for background subtraction.pdf:pdf},
  isbn      = {3540676864},
  issn      = {16113349},
  keywords  = {active real time, motion detection, non parametric estimation, shadow detection, vision, visual motion, visual surveillance},
  url       = {http://www.springerlink.com/index/3mcvhnwfa8bj4ln5.pdf{\%}5Cnhttp://link.springer.com/chapter/10.1007/3-540-45053-X{\_}48},
}

@Article{Mikolajczyk2005,
  author   = {Mikolajczyk, Krystian and Schmid, Cordelia},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {{A performance evaluation of local descriptors}},
  year     = {2005},
  issn     = {01628828},
  number   = {10},
  pages    = {1615--1630},
  volume   = {27},
  abstract = {In this paper, we compare the performance of descriptors computed for local interest regions, as, for example, extracted by the Harris-Affine detector [32]. Many different descriptors have been proposed in the literature. It is unclear which descriptors},
  doi      = {10.1109/TPAMI.2005.188},
  file     = {:D$\backslash$:/Users/farshid.pirahansiah/Dropbox/1396/myPaper/reference/Mikolajczyk2005 - A performance evaluation of local descriptors.pdf:pdf},
  isbn     = {0769519008},
  keywords = {Interest points, Interest regions, Invariance, Local descriptors, Matching, Recognition},
  pmid     = {16237996},
}

@inproceedings{Sivic2003,
abstract = {We describe an approach to object and scene retrieval which searches for and localizes all the occurrences of a user outlined object in a video. The object is represented by a set of viewpoint invariant region descriptors so that recognition can proceed successfully despite changes in viewpoint, illumination and partial occlusion. The temporal continuity of the video within a shot is used to track the regions in order to reject unstable regions and reduce the effects of noise in the descriptors. The analogy with text retrieval is in the implementation where matches on descriptors are pre-computed (using vector quantization), and inverted file systems and document rankings are used. The result is that retrieved is immediate, returning a ranked list of key frames/shots in the manner of Google. The method is illustrated for matching in two full length feature films.},
archivePrefix = {arXiv},
arxivId = {1504.06897},
author = {Sivic, Josef and Zisserman, Andrew},
booktitle = {Proceedings of the International Conference on Computer Vision},
doi = {10.1109/ICCV.2003.1238663},
eprint = {1504.06897},
file = {:D$\backslash$:/Users/farshid.pirahansiah/Dropbox/1396/myPaper/reference/Sivic2003 - Video google{\_} A text retrieval approach to object matching in videos..pdf:pdf},
isbn = {0769519504},
issn = {00189219},
number = {Iccv},
pages = {1470--1477},
pmid = {25052830},
title = {{Video Google: a text retrieval approach to object matching in videos}},
url = {http://ieeexplore.ieee.org/xpl/login.jsp?tp={\&}arnumber=1238663},
volume = {2},
year = {2003}
}

@Article{Kwak2014,
  author   = {Kwak, Hwan Joo and Park, Gwi Tae},
  journal  = {Journal of Intelligent Manufacturing},
  title    = {{Image contrast enhancement for intelligent surveillance systems using multi-local histogram transformation}},
  year     = {2014},
  issn     = {15728145},
  number   = {2},
  pages    = {303--318},
  volume   = {25},
  abstract = {Among all applications to monitor the safety and security of working environments, surveillance systems that use computer vision are the most efficient and intuitive in the manufacturing industry. This paper introduces a new technique of contrast enhancement for surveillance systems using computer vision. The histogram equalization method is a common and widespread image enhancement method which maximizes the contrast of the image. This contrast enhancement method usually improves the quality of images, but it can suffer from visual deterioration caused by excessive histogram modification. To overcome the limitations of conventional contrast enhancement methods, this paper introduces a new multi-local histogram transformation method for surveillance systems. This technique is based on the local histograms, which are separated from the overall histogram of the image, and the contrast of the image can be enhanced through two major processes: range reassignment of local histograms and local histogram equalization. The multi-local histogram transformation in this paper enhances the contrast of images, preventing excessive compression and extension of image histograms. The performance of the suggested contrast enhancement method is verified by the experiments in four different environments.},
  doi      = {10.1007/s10845-012-0663-4},
  isbn     = {0956-5515},
  keywords = {Histogram equalization (HE), Image enhancement, Multi-local histogram transformation (MLHT), Surveillance system},
  url      = {http://dx.doi.org/10.1007/s10845-012-0663-4},
}

@Article{Rowley1998,
  author   = {Rowley, Henry A. and Baluja, Shumeet and Kanade, Takeo},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {{Neural network-based face detection}},
  year     = {1998},
  issn     = {01628828},
  number   = {1},
  pages    = {23--38},
  volume   = {20},
  abstract = {idsia},
  doi      = {10.1109/34.655647},
  file     = {:D$\backslash$:/Users/farshid.pirahansiah/Dropbox/1396/myPaper/reference/Rowley1998 - Neural network-based face detection.pdf:pdf},
  isbn     = {0-8186-8497-6},
  keywords = {Artificial neural networks, Computer vision, Face detection, pattern recognition, Machine learning},
}

@article{Smeulders2000,
author = {Smeulders, A and Worring, M and Santini, S and Gupta, A and Jain, R},
journal = {IEEE Trans. On Pattern Analysis and Machine Intelligence},
number = {12},
pages = {1349--1380},
title = {{Content based image retrieval at the end of the early years}},
url = {http://ieeexplore.ieee.org/xpl/freeabs{\_}all.jsp?tp={\&}arnumber=895972{\&}isnumber=19391},
volume = {22(12)},
year = {2000}
}

@InProceedings{Cheung2011,
  author    = {Cheung, Brian and Sable, Carl},
  booktitle = {Proceedings - 10th International Conference on Machine Learning and Applications, ICMLA 2011},
  title     = {{Hybrid evolution of convolutional networks}},
  year      = {2011},
  month     = {dec},
  pages     = {293--297},
  volume    = {1},
  abstract  = {With the increasing trend of neural network models towards larger structures with more layers, we expect a corresponding exponential increase in the number of possible architectures. In this paper, we apply a hybrid evolutionary search procedure to define the initialization and architectural parameters of convolutional networks, one of the first successful deep network models. We make use of stochastic diagonal Levenberg-Marquardt to accelerate the convergence of training, lowering the time cost of fitness evaluation. Using parameters found from the evolutionary search together with absolute value and local contrast normalization preprocessing between layers, we achieve the best known performance on several of the MNIST Variations, rectangles-image and convex image datasets.},
  doi       = {10.1109/ICMLA.2011.73},
  isbn      = {9780769546070},
  keywords  = {convolutional networks, evolution, image classification, neural networks, second order methods},
}

@Article{Xia2016,
  author   = {Xia, Dao Xun and Su, Song Zhi and Geng, Li Chuan and Wu, Guo Xi and Li, Shao Zi},
  journal  = {Multimedia Systems},
  title    = {{Learning rich features from objectness estimation for human lying-pose detection}},
  year     = {2016},
  issn     = {09424962},
  pages    = {1--12},
  abstract = {Multimedia Systems, doi:10.1007/s00530-016-0518-5},
  doi      = {10.1007/s00530-016-0518-5},
  isbn     = {0053001605},
  keywords = {Deep model, Human lying-pose detection, Objectness estimation, Rich features learning, Saliency detection},
  url      = {http://dx.doi.org/10.1007/s00530-016-0518-5},
}

@article{Caselles1997,
author = {Caselles, V and Kimmel, R and Sapiro, G},
file = {:D$\backslash$:/Users/farshid.pirahansiah/Dropbox/1396/myPaper/reference/Caselles1997 - Geodesic active contours.pdf:pdf},
journal = {Ijcv},
number = {1},
pages = {61--79},
title = {{Geodesic Active Contours}},
url = {ftp://ftp-sop.inria.fr/athena/Team/Rachid.Deriche/Robotvis/Draft/GAC{\_}article.pdf},
volume = {22},
year = {1997}
}

@article{Krasin2016,
author = {Krasin, Ivan and Duerig, Tom and Alldrin, Neil and Veit, Andreas and Abu-El-Haija, Sami and Belongie, Serge and Cai, David and Feng, Zheyun and Ferrari, Vittorio and Gomes, Victor and Gupta, Abhinav and Narayanan, Dhyanesh and Sun, Chen and Chechik, Gal and Murphy, Kevin},
journal = {Dataset available from https://github.com/openimages},
title = {{OpenImages: A public dataset for large-scale multi-label and multi-class image classification.}},
year = {2016}
}

@InProceedings{Yoshioka2016,
  author    = {Yoshioka, T and Ohnishi, K and Fang, F and Nakatani, T},
  booktitle = {2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {{Noise robust speech recognition using recent developments in neural networks for computer vision}},
  year      = {2016},
  pages     = {5730--5734},
  abstract  = {Convolutional Neural Networks (CNNs) are superior to fully connected neural networks in various speech recognition tasks and the advantage is pronounced in noisy environments. In recent years, many techniques have been proposed in the computer vision community to improve CNN's classification performance. This paper considers two approaches recently developed for image classification and examines their impacts on noisy speech recognition performance. The first approach is to increase the depth of convolution layers. Different approaches to deepening the CNNs are compared. In particular, the usefulness of learning dynamic features with small convolution layers that perform convolution in time is shown along with a modulation frequency analysis of the learned convolution filters. The second approach is to use trainable activation functions. Specifically, the use of a Parametric Rectified Linear Unit (PReLU) is investigated. Experimental results show that both approaches yield significant improvements in performance. Combining the two approaches further reduces recognition errors, producing a word error rate of 11.1{\%} in the Aurora4 task, the best published result for this corpus, with a standard one-pass bi-gram decoding set-up.},
  doi       = {10.1109/ICASSP.2016.7472775},
  isbn      = {VO  -},
  issn      = {15206149},
  keywords  = {Acoustics, Automatic speech recognition, CNN, Convolution, Hidden Markov models, Neural networks, Noise measurement, PReLU, Speech recognition, Training, computer vision, convolution filters, convolutional neural network, convolutional neural networks, image classification, learning dynamic features, modulation frequency analysis, neural nets, noise robust speech recognition, noise robustness, parametric rectified linear unit, speech recognition},
}

@article{LeCun1989,
abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the US Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
doi = {10.1162/neco.1989.1.4.541},
file = {:D$\backslash$:/Users/farshid.pirahansiah/Dropbox/1396/myPaper/reference/LeCun1989 - Backpropagation applied to handwritten zip code recognition.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural Computation},
number = {4},
pages = {541--551},
pmid = {1000111957},
title = {{Backpropagation Applied to Handwritten Zip Code Recognition}},
volume = {1},
year = {1989}
}

@article{Freeman2002,
author = {Freeman, William T and Jones, Thouis R and Pasztor, Egon C},
file = {:D$\backslash$:/Users/farshid.pirahansiah/Dropbox/1396/myPaper/reference/Freeman2002 - Example-based super-resolution.pdf:pdf},
journal = {IEEE Computer graphics and Applications},
number = {April},
pages = {56--65},
title = {{Example-Based Super-Resolution}},
volume = {22},
year = {2002}
}

@Book{Duda2001,
  author    = {Duda, Richard O and Hart, Perter E and Stork, David G},
  publisher = {John Wiley {\&} Sons},
  title     = {{Pattern Classification}},
  year      = {2001},
  file      = {:D$\backslash$:/Users/farshid.pirahansiah/Dropbox/1396/myPaper/reference/Duda2012 - Pattern classification.pdf:pdf},
}

@Article{Sinha2014,
  author   = {Sinha, Atreyee and Banerji, Sugata and Liu, Chengjun},
  journal  = {Machine Vision and Applications},
  title    = {{New color GPHOG descriptors for object and scene image classification}},
  year     = {2014},
  issn     = {09328092},
  number   = {2},
  pages    = {361--375},
  volume   = {25},
  abstract = {This paper presents a novel set of image descriptors that encodes information from color, shape, spatial and local features of an image to improve upon the popular Pyramid of Histograms of Oriented Gradients (PHOG) descriptor for object and scene image classification. In particular, a new Gabor-PHOG (GPHOG) image descriptor created by enhancing the local features of an image using multiple Gabor filters is first introduced for feature extraction. Second, a comparative assessment of the classification performance of the GPHOG descriptor is made in grayscale and six different color spaces to further propose two novel color GPHOG descriptors that perform well on different object and scene image categories. Finally, an innovative Fused Color GPHOG (FC--GPHOG) descriptor is presented by integrating the Principal Component Analysis (PCA) features of the GPHOG descriptors in the six color spaces to combine color, shape and local feature information. Feature extraction for the proposed descriptors employs PCA and Enhanced Fisher Model (EFM), and the nearest neighbor rule is used for final classification. Experimental results using the MIT Scene dataset and the Caltech 256 object categories dataset show that the proposed new FC--GPHOG descriptor achieves a classification performance better than or comparable to other popular image descriptors, such as the Scale Invariant Feature Transform (SIFT) based Pyramid Histograms of visual Words descriptor, Color SIFT four Concentric Circles, Spatial Envelope, and Local Binary Patterns.},
  doi      = {10.1007/s00138-013-0561-6},
  keywords = {Color image search, FC-GPHOG, Gabor-PHOG (GPHOG), YCbCr-GPHOG, YIQ-GPHOG},
  url      = {http://dx.doi.org/10.1007/s00138-013-0561-6},
}

@Article{Cheng2016,
  author   = {Cheng, Gong and Zhou, Peicheng and Han, Junwei},
  journal  = {IEEE Transactions on Geoscience and Remote Sensing},
  title    = {{Learning Rotation-Invariant Convolutional Neural Networks for Object Detection in VHR Optical Remote Sensing Images}},
  year     = {2016},
  issn     = {01962892},
  month    = {dec},
  number   = {99},
  pages    = {1--11},
  volume   = {PP},
  abstract = {Object detection in very high resolution optical remote sensing images is a fundamental problem faced for remote sensing image analysis. Due to the advances of powerful feature representations, machine-learning-based object detection is receiving increasing attention. Although numerous feature representations exist, most of them are handcrafted or shallow-learning-based features. As the object detection task becomes more challenging, their description capability becomes limited or even impoverished. More recently, deep learning algorithms, especially convolutional neural networks (CNNs), have shown their much stronger feature representation power in computer vision. Despite the progress made in nature scene images, it is problematic to directly use the CNN feature for object detection in optical remote sensing images because it is difficult to effectively deal with the problem of object rotation variations. To address this problem, this paper proposes a novel and effective approach to learn a rotation-invariant CNN (RICNN) model for advancing the performance of object detection, which is achieved by introducing and learning a new rotation-invariant layer on the basis of the existing CNN architectures. However, different from the training of traditional CNN models that only optimizes the multinomial logistic regression objective, our RICNN model is trained by optimizing a new objective function via imposing a regularization constraint, which explicitly enforces the feature representations of the training samples before and after rotating to be mapped close to each other, hence achieving rotation invariance. To facilitate training, we first train the rotation-invariant layer and then domain-specifically fine-tune the whole RICNN network to further boost the performance. Comprehensive evaluations on a publicly available ten-class object detection data set demonstrate the effectiveness of the proposed method.},
  doi      = {10.1109/TGRS.2016.2601622},
  isbn     = {9781509014798},
  keywords = {Convolutional neural networks (CNNs), machine learning, object detection, remote sensing images, rotation-invariant CNN (RICNN)},
}

@Misc{PirahanSiah2020,
  author = {PirahanSiah, Farshid},
  month  = {apr},
  title  = {{Farshid PirahanSiah Website}},
  year   = {2020},
  editor = {PirahanSiah, Farshid},
  groups = {Farshid PirahanSiah},
  url    = {http://www.pirahansiah.com/},
}

@Book{Kaehler2015,
  author    = {Kaehler, Adrian and Bradski, Gary},
  publisher = {" OŔeilly Media, Inc."},
  title     = {{Learning OpenCV 3: Computer Vision in C++ with the OpenCV Library}},
  year      = {2015},
  isbn      = {1449314651, 9781449314651},
  groups    = {Book},
  pages     = {650},
}

@article{Griffin2007,
abstract = {We introduce a challenging set of 256 object categories containing a total of 30607 images. The original Caltech-101 1 was collected by choosing a set of object categories, downloading examples from Google Images and then manually screening out all images that did not fit the category. Caltech-256 is collected in a similar manner with several improvements: a) the number of categories is more than doubled, b) the minimum number of images in any category is increased from 31 to 80, c) aftifacts due to image rotation are avoided and d) a new and larger clutter category is introduced for testing background rejection. We suggest several testing paradigms to measure classification performance, the benchmark the dataset using two simple metrics as well as a state-of-the-art spatial pyramid matching 2 algorithm. Finally we use the clutter category to train an interest detector which rejects uninformative background regions.},
author = {Griffin, G and Holub, a and Perona, P},
isbn = {UCB/CSD-04-1366},
journal = {Caltech mimeo},
number = {1},
pages = {20},
title = {{Caltech-256 object category dataset}},
url = {http://authors.library.caltech.edu/7694},
volume = {11},
year = {2007}
}

@InProceedings{PirahanSiah2011,
  author    = {PirahanSiah, Farshid and Abdullah, Siti Norul Huda Sheikh and Sahran, Shahnorbanun},
  booktitle = {International Conference on Pattern Analysis and Intelligent Robotics},
  title     = {{Comparison single thresholding method for handwritten images segmentation}},
  year      = {2011},
  file      = {:farshid/conference/Comparison single thresholding method for handwritten images segmentation_2011.pdf:PDF},
  groups    = {Farshid PirahanSiah},
}

@InProceedings{Viola2001,
  author        = {Viola, P and Jones, M},
  booktitle     = {Computer Vision and Pattern Recognition (CVPR)},
  title         = {{Rapid object detection using a boosted cascade of simple features}},
  year          = {2001},
  pages         = {I----511----I----518},
  volume        = {1},
  abstract      = {This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the "integral image" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a "cascade" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1011.1669v3},
  doi           = {10.1109/CVPR.2001.990517},
  eprint        = {arXiv:1011.1669v3},
  file          = {:D$\backslash$:/Users/farshid.pirahansiah/Dropbox/1396/myPaper/reference/Viola2001 - Rapid object detection using a boosted cascade of simple features.pdf:pdf},
  isbn          = {0-7695-1272-0},
  issn          = {1063-6919},
  keywords      = {AdaBoost, Detectors, Face detection, Filters, Focusing, Image representation, Machine learning, Object detection, Pixel, Robustness, Skin, background regions, boosted simple feature cascade, classifiers, face detection, feature extraction, image classification, image processing, image representation, integral image, learning (artificial intelligence), machine learning, object detection, object specific focus-of-attention mechanism, rapid object detection, real-time applications, statistical guarantees, visual object detection},
  pmid          = {7143246},
}

@InProceedings{Kenny1997,
  author    = {Kenny, Owen P and Nelson, Douglas J and Meade, Fort George G},
  booktitle = {Optical Science, Engineering and Instrumentation9́7},
  title     = {{Time-frequency methods for enhancing speech}},
  year      = {1997},
  pages     = {48--57},
  volume    = {3162},
  doi       = {10.1117/12.284192},
  issn      = {0277786X},
  keywords  = {hermite polynomials, image enhancement, prolate-spheroidal fil-, singular value decomposition, speech enhancement, ter, time-frequency distribution, wiener filtering},
}

@article{Davtalab2014,
abstract = {In this paper a multi-level fuzzy min-max neural network classifier (MLF), which is a supervised learning method, is described. MLF uses basic concepts of the fuzzy min-max (FMM) method in a multi-level structure to classify patterns. This method uses separate classifiers with smaller hyperboxes in different levels to classify the samples that are located in overlapping regions. The final output of the network is formed by combining the outputs of these classifiers. MLF is capable of learning nonlinear boundaries with a single pass through the data. According to the obtained results, the MLF method, compared to the other FMM networks, has the highest performance and the lowest sensitivity to maximum size of the hyperbox parameter ($\theta$), with a training accuracy of 100{\%} in most cases.},
author = {Davtalab, R and Dezfoulian, M H and Mansoorizadeh, M},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {fuzzy neural nets;learning (artificial intelligenc},
number = {3},
pages = {470--482},
title = {{Multi-Level Fuzzy Min-Max Neural Network Classifier}},
volume = {25},
year = {2014}
}

@Article{Krizhevsky2012,
  author        = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal       = {Advances In Neural Information Processing Systems},
  title         = {{ImageNet Classification with Deep Convolutional Neural Networks}},
  year          = {2012},
  issn          = {10495258},
  month         = {dec},
  number        = {1},
  pages         = {1--9},
  volume        = {22},
  abstract      = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
  address       = {Washington, DC, USA},
  archiveprefix = {arXiv},
  arxivid       = {1102.0183},
  doi           = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
  editor        = {PirahanSiah, Farshid},
  eprint        = {1102.0183},
  isbn          = {9781627480031},
  keywords      = {1, 23, 3D context, 3d vision, Accuracy, Acoustics, AdaBoost, Anisotropic magnetoresistance, Automatic speech recognition, Bayes-Verfahren, Benchmark testing, Biological neural networks, CAR (Roboter), CNN, Camera calibration, Classification, Color image search, Computer Science, Computer architecture, Computer vision, Context, Contrast, Convolution, Correlation, DL-based active shape model, DLR, DSSD method, Decision support systems, Deep learning, Deep model, Detectors, Diffusion processes, Double-line rotation, Early vision, Equations, Error analysis, FC-GPHOG, FMM method, Face, Face alignment, Face detection, Face recognition, Farshid PirahanSiah, Feature description, Feature extraction, Filtering, Filters, Focusing, Gabor-PHOG (GPHOG), Graphics, Graphics processing unit, HDTV, Hardware, Hidden Markov models, High definition video, Histogram equalization (HE), Human lying-pose detection, Image color analysis, Image converters, Image edge detection, Image enhancement, Image generation, Image processing, Image representation, Image resolution, Image segmentation, Indexes, Interest points, Internet resource (url), Interpolation, Kernel, Learning systems, Local features, MLF, Machine learning, Mathematical model, Multi-local histogram transformation (MLHT), NTSC video content conversion, Nearest neighbor searches, Neural networks, Neurons, Noise measurement, Object detection, Object recognition, Objectness estimation, Optical character recognition, PReLU, Pixel, Predictive models, Probabilistischer Algorithmus, Probabilities, Probabilit{\'{e}}s, RGB-D, Redundancy, Rendering (computer graphics), Rich features learning, Roboter, Robotics, Robotique, Robustness, Saliency detection, Shape, Shearing, Single thresholding, Skin, Smoothing methods, Speech recognition, Streaming media, Surveillance system, TV, Three-dimensional displays, Training, Transforms, Unsicheres Schlie{\ss}en, Visualization, YCbCr-GPHOG, YIQ-GPHOG, active real time, anatomical object detection, anatomical pose estimation, and mapping, anisotropic diffusion, annotated image databases, aortic valve, automated feature design, background regions, biomedical ultrasonics, biometrics, boosted simple feature cascade, boundary delineation, character recognition, classifiers, clinical workflow, clustered high-probability regions, cml, comprehensive survey on object, computational efficiency, computer vision, concurrent mapping and, convolution filters, convolutional networks, convolutional neural network, convolutional neural network (CNN), convolutional neural networks, cs.LG, deep learning, deep learning network architectures, deep learning systems, deep sharable and structural detectors method, detection in optical remote, diagnosis, edge detection, evolution, example-based super-resolution, extensive dataset, eye fixation prediction, face alignment, face detection, face recognition, feature extraction, filter design image analysis, filtering and prediction theory, found in, full 3D data detection, full 3D data segmentation, fuzzy min-max, fuzzy neural nets, global feature, global shape regression, graphics, hermite polynomials, hierarchical marginal spaces, high frequency details, high-definition television, high-resolution enlargements, histogram, hog, humanoid robot, hyperbox, hyperboxes, image based rendering, image classification, image enhancement, image parsing, image processing, image representation, image resolution, image rotation, image sampling, image segmentation, image texture, image transform, image-based modeling, image-based rendering, image-based representations, integral image, interpolation, intraregion smoothing, learning (artificial intelligence), learning by example, learning classifiers, learning dynamic features, line rotation, localisation, localization, machine learning, machine learning methods, marginal space deep learning, marginal space learning, medical image processing, minimax techniques, modulation frequency analysis, motion detection, multilevel fuzzy min-max neural network classifier, multilevel structure, multitask learning framework, nearest-neighbor search, neighbouring landmarks, neural nets, neural networks, neurofuzzy, neuron, noise robust speech recognition, noise robustness, non parametric estimation, nonrigid object boundary, object detection, object localization, object parametrization, object specific focus-of-attention mechanism, ocr, of oriented gradients, optical character, orientation analysis, oriented filters, parallel processing, parametric rectified linear unit, parametrized representations, patient stratification, pattern classification, pattern clustering, pattern recognition, person identification, photo browsing, picture processing, pixel-based images, probability, prolate-spheroidal fil-, psnr, rapid object detection, real-time applications, regression analysis, remote sensing images, rendering (computer graphics), representative image features, restricted affine transformation, rihog, rotation-invariant hog, run-time performance, saliency detection., scale-space, scanning high-dimensional parametric spaces, scanning hypotheses, second order methods, segmentation support, sensing images can be, shadow detection, shape from shading, single thresholding, singular value decomposition, sparse adaptive data sampling patterns, sparse representations, speech enhancement, speech recognition, statistical guarantees, stereo vision on slam, structural feature learning method, structure from motion, supervised learning, supervised learning method, ter, texture analysis wavelets, texture mapping, therapy planning, three-dimensional (3D) object detection and segmen, threshold, time-frequency distribution, training-based super-resolution algorithm, two-step learning problem, ultrasonic imaging, ultrasound, vision, visual motion, visual object detection, visual simultaneous localization, visual surveillance, volumetric medical image data parsing, vslam, wiener filtering, zoomed images},
  pmid          = {7491034},
  publisher     = {MIT press},
}

@Article{FeiFei2004,
  author    = {Fei-Fei, Li and Fergus, Rod and Perona, Pietro},
  journal   = {Conference on Computer Vision and Pattern Recognition Workshop (CVPR 2004)},
  title     = {{Learning Generative Visual Models from Few Training Examples: An Incremental {\{}B{\}}ayesian Approach Tested on 101 Object Categories}},
  year      = {2004},
  number    = {1},
  pages     = {178},
  volume    = {106},
  doi       = {10.1109/CVPR.2004.109},
  isbn      = {VO  -},
  publisher = {Elsevier},
  url       = {http://dx.doi.org/10.1109/CVPR.2004.109},
}

@Misc{Nilsson1997,
  author        = {Nilsson, Nils J},
  title         = {{Introduction to Machine Learning}},
  year          = {1997},
  abstract      = {Machine learning and pattern recognition algorithms have in the past years developed to become a working horse in brain imaging and the computational neurosciences, as they are instrumental for mining vast amounts of neural data of ever increasing measurement precision and detecting minuscule signals from an overwhelming noise floor. They provide the means to decode and characterize task relevant brain states and to distinguish them from non-informative brain signals. While undoubtedly this machinery has helped to gain novel biological insights, it also holds the danger of potential unintentional abuse. Ideally machine learning techniques should be usable for any non-expert, however, unfortunately they are typically not. Overfitting and other pitfalls may occur and lead to spurious and nonsensical interpretation. The goal of this review is therefore to provide an accessible and clear introduction to the strengths and also the inherent dangers of machine learning usage in the neurosciences.},
  archiveprefix = {arXiv},
  arxivid       = {0904.3664v1},
  booktitle     = {Neural Networks},
  doi           = {10.1016/j.neuroimage.2010.11.004},
  eprint        = {0904.3664v1},
  isbn          = {0262012111},
  issn          = {10959572},
  number        = {2},
  pages         = {387--99},
  pmid          = {21172442},
  publisher     = {MIT press},
  url           = {http://robotics.stanford.edu/{~}nilsson/mlbook.html},
  volume        = {56},
}

@book{Szeliski2010,
abstract = {As humans, we perceive the three-dimensional structure of the world around us with apparent ease. However, despite all of the recent advances in computer vision research, the dream of having a computer interpret an image at the same level as a two-year old remains elusive. Why is computer vision such a challenging problem, and what is the current state of the art?Computer Vision: Algorithms and Applications explores the variety of techniques commonly used to analyze and interpret images. It also describes challenging real-world applications where vision is being successfully used, both for specialized applications such as medical imaging and fun consumer-level tasks such as image editing and stitching, which students can apply to their own personal photos and videos.More than just a source of "recipes", this text/reference also takes a scientific approach to basic vision problems, formulating physical models of the imaging process before inverting this process to produce the best possible descriptions of a scene. Exercises are presented throughout the book, with a heavy emphasis on testing algorithms.Suitable for either an undergraduate or a graduate-level course in computer vision, this textbook focuses on basic techniques that work under real-world conditions and encourages students to push their creative boundaries.Dr. Richard Szeliski has over twenty years' experience in computer vision research, most notably at Digital Equipment Corporation and Microsoft.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Szeliski, Richard},
doi = {10.1007/978-1-84882-935-0},
eprint = {arXiv:1011.1669v3},
isbn = {1848829345},
issn = {10636919},
pages = {870},
pmid = {16259003},
publisher = {Springer Science {\&} Business Media},
title = {{Computer Vision: Algorithms and Applications}},
url = {http://books.google.com/books?id=216LQgAACAAJ{\&}pgis=1},
year = {2010}
}

@Book{Prince2013,
  title         = {{Computer Vision: Models, Learning, and Inference}},
  publisher     = {Cambridge University Press},
  year          = {2013},
  author        = {Prince, Dr Simon J. D.},
  volume        = {12},
  number        = {4},
  isbn          = {9781107011793},
  abstract      = {This modern treatment of computer vision focuses on learning and inference in probabilistic models as a unifying theme. It shows how to use training data to learn the relationships between the observed image data and the aspects of the world that we wish to estimate, such as the 3D structure or the object class, and how to exploit these relationships to make new inferences about the world from new image data. With minimal prerequisites, the book starts from the basics of probability and model fitting and works up to real examples that the reader can implement and modify to build useful vision systems. Primarily meant for advanced undergraduate and graduate students, the detailed methodological presentation will also be useful for practitioners of computer vision. - Covers cutting-edge techniques, including graph cuts, machine learning, and multiple view geometry. - A unified approach shows the common basis for solutions of important computer vision problems, such as camera calibration, face recognition, and object tracking. - More than 70 algorithms are described in sufficient detail to implement. - More than 350 full-color illustrations amplify the text. - The treatment is self-contained, including all of the background mathematics. - Additional resources at www.computervisionmodels.com.},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1011.1669v3},
  booktitle     = {The Lancet Neurology},
  doi           = {10.1016/S1474-4422(13)70064-4},
  eprint        = {arXiv:1011.1669v3},
  issn          = {14744422},
  pages         = {335},
  pmid          = {2013185901},
  url           = {http://linkinghub.elsevier.com/retrieve/pii/S1474442213700644},
}

@book{Nixon2012,
abstract = {This book is an essential guide to the implementation of image processing and computer vision techniques, with tutorial introductions and sample code in Matlab. Algorithms are presented and fully explained to enable complete understanding of the methods and techniques demonstrated. As one reviewer noted, "The main strength of the proposed book is the exemplar code of the algorithms." Fully updated with the latest developments in feature extraction, including expanded tutorials and new techniques, this new edition contains extensive new material on Haar wavelets, Viola-Jones, bilateral filtering, SURF, PCA-SIFT, moving object detection and tracking, development of symmetry operators, LBP texture analysis, Adaboost, and a new appendix on color models. Coverage of distance measures, feature detectors, wavelets, level sets and texture tutorials has been extended. Named a 2012 Notable Computer Book for Computing Methodologies by Computing ReviewsEssential reading for engineers and students working in this cutting-edge fieldIdeal module text and background reference for courses in image processing and computer visionThe only currently available text to concentrate on feature extraction with working implementation and worked through derivation},
author = {Nixon, Mark S. and Aguado, Alberto S.},
doi = {http://dx.doi.org/10.1016/B978-0-12-396549-3.00010-0},
isbn = {978-0-12-396549-3},
pages = {181--182},
publisher = {Academic Press},
title = {{Feature Extraction {\&} Image Processing for Computer Vision}},
url = {https://books.google.com/books?id=lytnomY-r7YC},
year = {2012}
}

@Article{Karen2015,
  author        = {Karen, Simonyan and Andrew, Zisserman},
  title         = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
  year          = {2015},
  pages         = {1--14},
  issn          = {09505849},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1409.1556v6},
  doi           = {10.1016/j.infsof.2008.09.005},
  eprint        = {arXiv:1409.1556v6},
  isbn          = {0950-5849},
  pmid          = {16873662},
}

@article{Lucas1981,
author = {Lucas, Bruce D},
journal = {Imaging},
pages = {121--129},
publisher = {Vancouver, BC, Canada},
title = {{An Iterative Image Registration Technique with an Application to Stereo Vision}},
volume = {130},
year = {1981}
}

@Article{Bay2008,
  author   = {Bay, Herbert and Ess, Andreas and Tuytelaars, Tinne and {Van Gool}, Luc},
  journal  = {Computer Vision and Image Understanding},
  title    = {{Speeded-Up Robust Features (SURF)}},
  year     = {2008},
  issn     = {10773142},
  number   = {3},
  pages    = {346--359},
  volume   = {110},
  abstract = {This article presents a novel scale- and rotation-invariant detector and descriptor, coined SURF (Speeded-Up Robust Features). SURF approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (specifically, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper encompasses a detailed description of the detector and descriptor and then explores the effects of the most important parameters. We conclude the article with SURF's application to two challenging, yet converse goals: camera calibration as a special case of image registration, and object recognition. Our experiments underline SURF's usefulness in a broad range of topics in computer vision. ?? 2007 Elsevier Inc. All rights reserved.},
  doi      = {10.1016/j.cviu.2007.09.014},
  isbn     = {9783540338321},
  keywords = {Camera calibration, Feature description, Interest points, Local features, Object recognition},
  pmid     = {16081019},
}

@inproceedings{Gortler1996,
author = {Gortler, Steven J. and Grzeszczuk, Radek and Szeliski, Richard and Cohen, Michael F.},
booktitle = {the 23rd annual conference on Computer graphics and interactive techniques},
doi = {10.1145/237170.237200},
isbn = {0897917464},
issn = {00978930},
organization = {ACM},
pages = {43--54},
title = {{The lumigraph}},
url = {http://portal.acm.org/citation.cfm?doid=237170.237200},
year = {1996}
}

@InProceedings{Snavely2006,
  author       = {Snavely, Noah and Seitz, Steven M and Szeliski, Richard},
  booktitle    = {ACM Transactions on Graphics},
  title        = {{Photo tourism: Exploring Photo Collections in 3D}},
  year         = {2006},
  number       = {3},
  organization = {ACM},
  pages        = {835--846},
  volume       = {25},
  abstract     = {We present a system for interactively browsing and exploring large unstructured collections of photographs of a scene using a novel 3D interface. Our system consists of an image-based modeling front end that automatically computes the viewpoint of each photograph as well as a sparse 3D model of the scene and image to model correspondences. Our photo explorer uses image-based rendering techniques to smoothly transition between photographs, while also enabling full 3D navigation and exploration of the set of images and world geometry, along with auxiliary information such as overhead maps. Our system also makes it easy to construct photo tours of scenic or historic locations, and to annotate image details, which are automatically transferred to other relevant images. We demonstrate our system on several large personal photo collections as well as images gathered from Internet photo sharing sites.},
  doi          = {10.1145/1141911.1141964},
  isbn         = {1595933646},
  issn         = {07300301},
  keywords     = {image-based modeling, image-based rendering, photo browsing, structure from motion},
  pmid         = {18787244},
  url          = {http://doi.acm.org/10.1145/1141911.1141964{\%}5Cnhttp://portal.acm.org/citation.cfm?doid=1141911.1141964},
}

@article{Kass1988,
author = {Kass, M and Witkin, A and Terzopoulos, D},
journal = {International Journal on Computer Vision},
number = {4},
pages = {321--331},
title = {{Snakes: active contour model}},
volume = {1},
year = {1988}
}

@Article{RMHaralickIDinstein1973,
  author    = {{R M Haralick, I Dinstein}, K Shanmugam},
  title     = {{Textural features for image classification}},
  year      = {1973},
  volume    = {3},
  number    = {6},
  pages     = {610--621},
  publisher = {Ieee},
}

@Article{Zhao2003,
  author    = {Zhao, W. and Chellappa, R. and Phillips, P. J. and Rosenfeld, a. and Zhao, W. and Chellappa, R. and Chellappa, R. and Phillips, P. J. and Phillips, P. J. and Rosenfeld, a. and Rosenfeld, a.},
  journal   = {ACM Comput. Surv.},
  title     = {{Face Recognition: A Literature Survey}},
  year      = {2003},
  issn      = {0360-0300},
  number    = {4},
  pages     = {399--458},
  volume    = {35},
  abstract  = {As one of the most successful applications of image analysis and understanding, face recognition has recently received significant attention, especially during the past several years. At least two reasons account for this trend: the first is the wide range of commercial and law enforcement applications, and the second is the availability of feasible technologies after 30 years of research. Even though current machine recognition systems have reached a certain level of maturity, their success is limited by the conditions imposed by many real applications. For example, recognition of face images acquired in an outdoor environment with changes in illumination and/or pose remains a largely unsolved problem. In other words, current systems are still far away from the capability of the human perception system.This paper provides an up-to-date critical survey of still- and video-based face recognition research. There are two underlying motivations for us to write this survey paper: the first is to provide an up-to-date review of the existing literature, and the second is to offer some insights into the studies of machine recognition of faces. To provide a comprehensive survey, we not only categorize existing recognition techniques but also present detailed descriptions of representative methods within each category. In addition, relevant topics such as psychophysical studies, system evaluation, and issues of illumination and pose variation are covered.},
  doi       = {10.1145/954339.954342},
  isbn      = {0360-0300},
  keywords  = {Face recognition, person identification},
  pmid      = {19556198},
  publisher = {ACM},
  url       = {http://www.face-rec.org/interesting-papers/General/zhao00face.pdf{\%}5Cnhttp://portal.acm.org/citation.cfm?id=954342{\%}5Cnhttp://doi.acm.org/10.1145/954339.954342},
}

@article{Freeman1991,
author = {Freeman, William T and Adelson, Edward H},
journal = {Technology},
number = {9},
pages = {891--906},
title = {{The Design and Use of Steerable Filters Copyright 1 Introduction}},
volume = {13},
year = {1991}
}

@Misc{Thrun2005,
  author        = {Thrun, Sebastian and Burgard, Wolfram and Fox, Dieter},
  title         = {{Probabilistic robotics}},
  year          = {2005},
  abstract      = {Probablistic robotics is a growing area in the subject, concerned with perception and control in the face of uncertainty and giving robots a level of robustness in real-world situations. This book introduces techniques and algorithms in the field.; Recursive state estimation -- Gaussian filters -- Robot motion -- Robot perception -- Mobile robot localization : Markov and Gaussian -- Mobile robot localization : grid and Monte Carlo -- Occupancy grid mapping -- Simultaneous localization and mapping -- The graphSLAM algorithm -- The sparse extended information filter -- The fastSLAM algorithm -- Markov decision processes -- Partially observable Markov decision processes -- Approximate POMDP techniques -- Exploration.},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1011.1669v3},
  booktitle     = {Intelligent robotics and autonomous agents; Variation: Intelligent robotics and autonomous agents.},
  doi           = {10.1145/504729.504754},
  eprint        = {arXiv:1011.1669v3},
  isbn          = {0262201623 (alk. paper); 9780262201629 (alk. paper)},
  issn          = {00010782},
  keywords      = {Bayes-Verfahren, CAR (Roboter), Internet resource (url), Probabilistischer Algorithmus, Probabilities, Probabilit{\'{e}}s, Roboter, Robotics, Robotique, Unsicheres Schlie{\ss}en},
  pages         = {647},
  pmid          = {25246403},
  publisher     = {MIT Press},
}

@Article{Breitenreiter2015,
  author        = {Breitenreiter, Anselm and Poppinga, Heiko and Berlin, T U and Technik, Fachgebiet Nachrichten},
  title         = {{Deep Learning}},
  year          = {2015},
  volume        = {521},
  number        = {7553},
  pages         = {2015},
  issn          = {1548-7091},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1312.6184v5},
  doi           = {10.1038/nmeth.3707},
  eprint        = {arXiv:1312.6184v5},
  isbn          = {9780521835688},
  pmid          = {10463930},
  publisher     = {Nature Research},
}

@Article{Pirahansiah2013a,
  author    = {Pirahansiah, Farshid and Abdullah, Siti Norul Huda Sheikh and Sahran, Shahnorbanun},
  journal   = {Journal of Theoretical and Applied Information Technology},
  title     = {{Peak signal-to-noise ratio based on threshold method for image segmentation}},
  year      = {2013},
  issn      = {18173195},
  number    = {2},
  pages     = {158--168},
  volume    = {57},
  file      = {:farshid/Journals/PEAK SIGNAL-TO-NOISE RATIO BASED ON THRESHOLD METHOD FOR IMAGE SEGMENTATION_2013.pdf:PDF},
  groups    = {Farshid PirahanSiah},
  keywords  = {Image processing, Image segmentation, Optical character recognition, PSNR, Single thresholding},
  publisher = {www.jatit.org/volumes/Vol57No2/4Vol57No2.pdf},
}

@Article{Naeimizaghiani2011,
  author   = {Naeimizaghiani, Maryam and Abdullah, Siti Norul Huda Sheikh and Bataineh, Bilal and PirahanSiah, Farshid},
  journal  = {Proceedings of the 2011 International Conference on Electrical Engineering and Informatics},
  title    = {{Character recognition based on global feature extraction}},
  year     = {2011},
  issn     = {21556822},
  number   = {2},
  pages    = {1--4},
  volume   = {52},
  abstract = {This paper presents a enhanced feature extraction method which is a combination and selected of two feature extraction techniques of Gray Level Co occurrence Matrix (GLCM) and Edge Direction Matrixes (EDMS) for character recognition purpose. It is apparent that one of the most important steps in a character recognition system is selecting a better feature extraction technique, while the variety of method makes difficulty for finding the best techniques for character recognition. The dataset of images that has been applied to the different feature extraction techniques includes the binary character with different sizes. Experimental results show the better performance of proposed method in compared with GLCM and EDMS method after performing the feature selection with neural network, bayes network and decision tree classifiers},
  file     = {:farshid/conference/Character Recognition Based on Global Feature_2011.pdf:PDF},
  groups   = {Farshid PirahanSiah},
  isbn     = {9781457707513},
  keywords = {character recognition, feature extraction, image processing, ocr},
  url      = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5665123},
}

@InProceedings{Alex2012,
  author    = {Alex, Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E.},
  title     = {{Imagenet classification with deep convolutional neural networks}},
  booktitle = {Neural Information Processing Systems (NIPS)},
  year      = {2012},
  pages     = {1097--1105},
  url       = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
}

@Article{Schmidhuber2012,
  author        = {Schmidhuber, Jurgen},
  journal       = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR),},
  title         = {{Multi-column deep neural networks for image classification}},
  year          = {2012},
  pages         = {3642--3649},
  abstract      = {Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible, wide and deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1202.2745v1},
  doi           = {10.1109/CVPR.2012.6248110},
  eprint        = {arXiv:1202.2745v1},
  isbn          = {978-1-4673-1226-4},
  keywords      = {Benchmark testing, Computer architecture, Error analysis, Graphics processing unit, Neurons, Training},
  url           = {http://dl.acm.org/citation.cfm?id=2354409.2354694},
}

@article{Haralick1979,
abstract = {In this survey we review the image processing literature on the various approaches and models investigators have used for texture. These include statistical approaches of autocorrelation function, optical transforms, digital transforms, textural edgeness, structural element, gray tone cooccurrence, run lengths, and autoregressive models. We discuss and generalize some structural approaches to texture based on more complex primitives than gray tone. We conclude with some structural-statistical generalizations which apply the statistical techniques to the structural primitives.},
author = {Haralick, Robert M.},
doi = {10.1109/PROC.1979.11328},
isbn = {0018-9219},
issn = {15582256},
journal = {Proceedings of the IEEE},
number = {5},
pages = {786--804},
publisher = {IEEE},
title = {{Statistical and structural approaches to texture}},
volume = {67},
year = {1979}
}

@Article{Intelligence2003,
  author  = {Intelligence, A},
  title   = {{A modern approach}},
  journal = {Russell and Norvig},
  year    = {2003},
  volume  = {25},
  pages   = {1--5},
  isbn    = {0131038052},
  url     = {http://scholar.google.com/scholar?hl=en{\&}q=artificial+intelligence+A+Modern+Approach{\&}btnG={\&}as{\_}sdt=1,5{\&}as{\_}sdtp={\#}2},
}

@book{Trucco1998,
author = {Trucco, Emanuele and Alessandro, Verri},
booktitle = {Prentice-Hall.},
isbn = {0132611082},
publisher = {Prentice Hall Englewood Cliffs},
title = {{Introductory Techniques for 3D Computer Vision}},
volume = {201},
year = {1998}
}

@Article{LeCun1998,
  author        = {LeCun, Yann and Bottou, L??on and Bengio, Yoshua and Haffner, Patrick},
  journal       = {Proceedings of the IEEE},
  title         = {{Gradient-based learning applied to document recognition}},
  year          = {1998},
  issn          = {00189219},
  number        = {11},
  pages         = {2278--2323},
  volume        = {86},
  abstract      = {Multilayer neural networks trained with the back-propagation$\backslash$nalgorithm constitute the best example of a successful gradient based$\backslash$nlearning technique. Given an appropriate network architecture,$\backslash$ngradient-based learning algorithms can be used to synthesize a complex$\backslash$ndecision surface that can classify high-dimensional patterns, such as$\backslash$nhandwritten characters, with minimal preprocessing. This paper reviews$\backslash$nvarious methods applied to handwritten character recognition and$\backslash$ncompares them on a standard handwritten digit recognition task.$\backslash$nConvolutional neural networks, which are specifically designed to deal$\backslash$nwith the variability of 2D shapes, are shown to outperform all other$\backslash$ntechniques. Real-life document recognition systems are composed of$\backslash$nmultiple modules including field extraction, segmentation recognition,$\backslash$nand language modeling. A new learning paradigm, called graph transformer$\backslash$nnetworks (GTN), allows such multimodule systems to be trained globally$\backslash$nusing gradient-based methods so as to minimize an overall performance$\backslash$nmeasure. Two systems for online handwriting recognition are described.$\backslash$nExperiments demonstrate the advantage of global training, and the$\backslash$nflexibility of graph transformer networks. A graph transformer network$\backslash$nfor reading a bank cheque is also described. It uses convolutional$\backslash$nneural network character recognizers combined with global training$\backslash$ntechniques to provide record accuracy on business and personal cheques.$\backslash$nIt is deployed commercially and reads several million cheques per day$\backslash$n},
  archiveprefix = {arXiv},
  arxivid       = {1102.0183},
  doi           = {10.1109/5.726791},
  eprint        = {1102.0183},
  isbn          = {0018-9219},
  keywords      = {Convolutional neural networks, Document recognition, Finite state transducers, Gradient-based learning, Graph transformer networks, Machine learning, Neural networks, Optical character recognition (OCR)},
  pmid          = {15823584},
  publisher     = {IEEE},
}

@Article{Hinton2006,
  author        = {Hinton, GE and Osindero, Simon and Teh, YW},
  journal       = {Neural computation},
  title         = {{A fast learning algorithm for deep belief nets}},
  year          = {2006},
  issn          = {0899-7667},
  month         = {may},
  number        = {7},
  pages         = {1527--1554},
  volume        = {18},
  abstract      = {We show how to use complementary priors to eliminate the explaining- away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associa- tive memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive ver- sion of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribu- tion of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning al- gorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
  annote        = {doi: 10.1162/neco.2006.18.7.1527},
  archiveprefix = {arXiv},
  arxivid       = {1111.6189v1},
  doi           = {10.1162/neco.2006.18.7.1527},
  eprint        = {1111.6189v1},
  file          = {:Hinton2006 - A fast learning algorithm for deep belief nets.1527:},
  groups        = {Markings},
  isbn          = {0899-7667},
  pmid          = {16764513},
  publisher     = {MIT Press},
  url           = {http://www.mitpressjournals.org/doi/abs/10.1162/neco.2006.18.7.1527},
}

@Article{Liu2016,
  author   = {N. Liu and J. Han and T. Liu and X. Li},
  title    = {Learning to Predict Eye Fixations via Multiresolution Convolutional Neural Networks},
  journal  = {IEEE Transactions on Neural Networks and Learning Systems},
  year     = {2016},
  volume   = {PP},
  number   = {99},
  pages    = {1-13},
  issn     = {2162-237X},
  abstract = {Eye movements in the case of freely viewing natural scenes are believed to be guided by local contrast, global contrast, and top-down visual factors. Although a lot of previous works have explored these three saliency cues for several years, there still exists much room for improvement on how to model them and integrate them effectively. This paper proposes a novel computation model to predict eye fixations, which adopts a multiresolution convolutional neural network (Mr-CNN) to infer these three types of saliency cues from raw image data simultaneously. The proposed Mr-CNN is trained directly from fixation and nonfixation pixels with multiresolution input image regions with different contexts. It utilizes image pixels as inputs and eye fixation points as labels. Then, both the local and global contrasts are learned by fusing information in multiple contexts. Meanwhile, various top-down factors are learned in higher layers. Finally, optimal combination of top-down factors and bottom-up contrasts can be learned to predict eye fixations. The proposed approach significantly outperforms the state-of-the-art methods on several publically available benchmark databases, demonstrating the superiority of Mr-CNN. We also apply our method to the RGB-D image saliency detection problem. Through learning saliency cues induced by depth and RGB information on pixel level jointly and their interactions, our model achieves better performance on predicting eye fixations in RGB-D images.},
  doi      = {10.1109/TNNLS.2016.2628878},
  keywords = {Context;Feature extraction;Image color analysis;Image resolution;Predictive models;Streaming media;Visualization;Contrast;RGB-D;convolutional neural network (CNN);eye fixation prediction;saliency detection.},
}

@InProceedings{He2015,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  title     = {Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  booktitle = {Proceedings of the IEEE international conference on computer vision},
  year      = {2015},
  pages     = {1026--1034},
}

@Article{Xu2015,
  author  = {Xu, Bing and Wang, Naiyan and Chen, Tianqi and Li, Mu},
  title   = {Empirical evaluation of rectified activations in convolutional network},
  journal = {arXiv preprint arXiv:1505.00853},
  year    = {2015},
}

@Article{Srivastava2014,
  author  = {Srivastava, Nitish and Hinton, Geoffrey E and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  title   = {Dropout: a simple way to prevent neural networks from overfitting.},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {1},
  pages   = {1929--1958},
}

@InProceedings{Wei2015,
  author    = {Wei, Xiu-Shen and Gao, Bin-Bin and Wu, Jianxin},
  title     = {Deep spatial pyramid ensemble for cultural event recognition},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision Workshops},
  year      = {2015},
  pages     = {38--44},
}

@Book{Zhou2012,
  title     = {Ensemble methods: foundations and algorithms},
  publisher = {CRC press},
  year      = {2012},
  author    = {Zhou, Zhi-Hua},
}

@Misc{Masko2015,
  author = {Masko, David and Hensman, Paulina},
  title  = {The impact of imbalanced training data for convolutional neural networks},
  year   = {2015},
}

@InCollection{Goodfellow2014,
  author    = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  title     = {Generative Adversarial Nets},
  booktitle = {Advances in Neural Information Processing Systems 27},
  publisher = {Curran Associates, Inc.},
  year      = {2014},
  editor    = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
  pages     = {2672--2680},
  file      = {:pdf/5423-generative-adversarial-nets.pdf:PDF},
  url       = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
}

@InCollection{Denton2015,
  author    = {Denton, Emily L and Chintala, Soumith and szlam, arthur and Fergus, Rob},
  title     = {Deep Generative Image Models using a ￼Laplacian Pyramid of Adversarial Networks},
  booktitle = {Advances in Neural Information Processing Systems 28},
  publisher = {Curran Associates, Inc.},
  year      = {2015},
  editor    = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
  pages     = {1486--1494},
  url       = {http://papers.nips.cc/paper/5773},
}

@Article{Hajigholam2016,
  author    = {Hajigholam, Mohammad-Hossein and Raie, Abolghasem-Asadollah and Faez, Karim},
  title     = {Multitask joint spatial pyramid matching using sparse representation with dynamic coefficients for object recognition},
  journal   = {Journal of Electronic Imaging},
  year      = {2016},
  volume    = {25},
  number    = {2},
  pages     = {023019--023019},
  publisher = {International Society for Optics and Photonics},
}

@Article{Banerji2013,
  author    = {Banerji, Sugata and Sinha, Atreyee and Liu, Chengjun},
  title     = {New image descriptors based on color, texture, shape, and wavelets for object and scene image classification},
  journal   = {Neurocomputing},
  year      = {2013},
  volume    = {117},
  pages     = {173--185},
  publisher = {Elsevier},
}

@InProceedings{Bosch2007,
  author       = {Bosch, Anna and Zisserman, Andrew and Munoz, Xavier},
  title        = {Image classification using random forests and ferns},
  booktitle    = {Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on},
  year         = {2007},
  pages        = {1--8},
  organization = {IEEE},
}

@Article{Torralba2008,
  author    = {Torralba, Antonio and Fergus, Rob and Freeman, William T},
  title     = {80 million tiny images: A large data set for nonparametric object and scene recognition},
  journal   = {IEEE transactions on pattern analysis and machine intelligence},
  year      = {2008},
  volume    = {30},
  number    = {11},
  pages     = {1958--1970},
  publisher = {IEEE},
}

@InProceedings{Sinha2012,
  author       = {Sinha, Atreyee and Banerji, Sugata and Liu, Chengjun},
  title        = {Novel color Gabor-LBP-PHOG (GLP) descriptors for object and scene image classification},
  booktitle    = {Proceedings of the Eighth Indian Conference on Computer Vision, Graphics and Image Processing},
  year         = {2012},
  pages        = {58},
  organization = {ACM},
}

@Article{VanGemert2010,
  author    = {Van Gemert, Jan C and Veenman, Cor J and Smeulders, Arnold WM and Geusebroek, Jan-Mark},
  title     = {Visual word ambiguity},
  journal   = {IEEE transactions on pattern analysis and machine intelligence},
  year      = {2010},
  volume    = {32},
  number    = {7},
  pages     = {1271--1283},
  publisher = {IEEE},
}

@Article{Chen2017,
  author  = {Chen, Weihua and Chen, Xiaotang and Zhang, Jianguo and Huang, Kaiqi},
  title   = {Beyond triplet loss: a deep quadruplet network for person re-identification},
  journal = {arXiv preprint arXiv:1704.01719},
  year    = {2017},
}

@Article{Drummond2003,
  author    = {Drummond, Chris and C. Holte, Robert},
  title     = {C4.5, Class Imbalance, and Cost Sensitivity: Why Under-Sampling beats OverSampling},
  year      = {2003},
  month     = {01},
  booktitle = {Proceedings of the ICML'03 Workshop on Learning from Imbalanced Datasets},
}

@Unknown{Zadrozny2003,
  author    = {Zadrozny, Bianca and Langford, J and Abe, Naoki},
  title     = {Cost-Sensitive Learning by Cost-Proportionate Example Weighting},
  month     = {12},
  year      = {2003},
  booktitle = {Proceedings - IEEE International Conference on Data Mining, ICDM},
  isbn      = {0-7695-1978-4},
  pages     = {435- 442},
}

@Article{Simonyan2014,
  author  = {Simonyan, Karen and Zisserman, Andrew},
  title   = {Very deep convolutional networks for large-scale image recognition},
  journal = {arXiv preprint arXiv:1409.1556},
  year    = {2014},
}

@InProceedings{Pirahansiah2015,
  author    = {F. {Pirahansiah} and S. N. H. S. {Abdullah} and S. {Sahran}},
  booktitle = {2015 10th Asian Control Conference (ASCC)},
  title     = {Camera calibration for multi-modal robot vision based on image quality assessment},
  year      = {2015},
  month     = {May},
  pages     = {1-6},
  abstract  = {Multi-dimension robot vision in autonomous humanoid robot is still an open issue as it performs less effective when dealing with different environments. Robot vision becomes more challenging as image quality degrades. Unlike human vision, current robot vision is yet to calibrate automatically when image quality changes abruptly. This may result in poor accuracy due to false negative input data points, and the user needs recapturing new calibration images to compensate. Therefore, this study emphasizes on proposing an automatic calibration for multimodal robot vision based on quality measures. We organize our research methodology into three steps. First, we capture a series of image patterns by using our calibration pattern equipment. Second, we employ Image Quality Assessment Function (IQAF) that includes PSNR and SSIM to measure points of image abruption simultaneously. In the experiment, we observed differences between real distance and computed distance and compared them to those of the selfcollected original database and the blur database.},
  doi       = {10.1109/ASCC.2015.7360336},
  file      = {:farshid/conference/Camera Calibration for Multi-Modal Robot Vision_2015.pdf:PDF},
  groups    = {Farshid PirahanSiah},
  keywords  = {calibration;cameras;humanoid robots;robot vision;multimodal robot vision;autonomous humanoid robot;human vision;calibration images;automatic camera calibration;image patterns;calibration pattern equipment;image quality assessment function;IQAF;PSNR;SSIM;image abruption;self-collected original database;blur database;Cameras;Calibration;Robot vision systems;Image quality;Three-dimensional displays;Stereo vision;Mathematical model;stereopsis calibration;binocular vision;3D vision;stereo vision;humanoid robot;Camera re-sectioning;geometric Camera Calibration;create datasets},
}

@InProceedings{PirahanSiah2017,
  author    = {F. {PirahanSiah} and S. N. H. S. {Abdullah} and S. {Sahran}},
  booktitle = {2017 IEEE 15th Student Conference on Research and Development (SCOReD)},
  title     = {Pattern image significance for camera calibration},
  year      = {2017},
  month     = {Dec},
  pages     = {456-460},
  abstract  = {The image information from cameras can yield geometric information pertaining to three-dimensional objects by having camera parameters. Camera Calibration is a method for calculating the parameters of a pinhole camera model. Several methods used for camera calibration which are self-calibration, active vision, and known objects. Usually known object pattern uses calibration pattern such as chessboard. Furthermore, another important element is the number of image selection that also adheres better impact to overall of accuracy rate. Therefore, we categorize and explain each method in camera calibration in this paper. Finally, we show the significance of number of image and slope in camera calibration in several experimental result to justify our claim.},
  doi       = {10.1109/SCORED.2017.8305440},
  file      = {:farshid/conference/Pattern Image Significance for Camera Calibration-2017.pdf:PDF},
  groups    = {Farshid PirahanSiah},
  keywords  = {active vision;calibration;cameras;stereo image processing;pattern image significance;camera calibration;camera parameters;pinhole camera model;3D objects;image selection;active vision;Calibration;Cameras;Distortion;Three-dimensional displays;Robot vision systems;Conferences;Research and development;Camera Calibration;calibration pattern;adaptive parameter setting;3D;reconstruction;geometry},
}

@Article{Asgari2017,
  author  = {Asgari, Maryam and Pirahansiah, Farshid and Shahverdy, Mohammad and Fartash, Mehdi},
  journal = {Journal of Theoretical and Applied Information Technology},
  title   = {Using an ant colony optimization algorithm for image edge detection as a threshold segmentation for OCR system},
  year    = {2017},
  number  = {21},
  pages   = {5654--5664},
  volume  = {95},
  file    = {:farshid/Journals/USING AN ANT COLONY OPTIMIZATION ALGORITHM_2017_1Vol95No21.pdf:PDF},
  groups  = {Farshid PirahanSiah},
}

@Article{Naeimizaghiani2013,
  author  = {Naeimizaghiani, Maryam and Abdullah, Siti Norul Huda Sheikh and Pirahansiah, Farshid and Bataineh, Bilal},
  journal = {Journal of Theoretical \& Applied Information Technology},
  title   = {{Character and Object Recognition Based on Global Feature Extraction}},
  year    = {2013},
  number  = {2},
  volume  = {52},
  file    = {:farshid/Journals/CHARACTER AND OBJECT RECOGNITION BASED ON GLOBAL FEATURE EXTRACTION_2013.pdf:PDF},
  groups  = {Farshid PirahanSiah},
}

@InProceedings{ZainalAbidin2011,
  author    = {N. H. {Zainal Abidin} and S. N. H. S. {Abdullah} and S. {Sahran} and F. {PirahanSiah}},
  booktitle = {Proceedings of the 2011 International Conference on Electrical Engineering and Informatics},
  title     = {License plate recognition with multi-threshold based on entropy},
  year      = {2011},
  month     = {July},
  pages     = {1-6},
  abstract  = {Among all the existing segmentation techniques, thresholding technique is one of the most popular one due to its simplicity, robustness and accuracy. Multi-thresholding is an important operation in many analyses which is used in many applications. Selecting correct thresholds to get better result is a critical issue. In this research, a multilevel thresholding method is proposed based on combination of maximum entropy. The maximum entropy thresholding algorithm selects several threshold values by maximizing the cross entropy between the original image and the segmented image. This method can effectively integrate partial range of the image histogram. The proposed algorithm is compared with single thresholding method based on maximum entropy and multilevel thresholding method The proposed multi thresholding method is tested on license plate application. From the experiment, multi-threshold method further improved to increase the segmentation accuracy in the future.},
  doi       = {10.1109/ICEEI.2011.6021627},
  file      = {:farshid/conference/License Plate Recognition with Multi-threshold_2011.pdf:PDF},
  groups    = {Farshid PirahanSiah},
  keywords  = {image segmentation;maximum entropy methods;optical character recognition;license plate recognition;multithreshold technique;multilevel thresholding method;maximum entropy thresholding algorithm;original image;image segmentation;image histogram;single thresholding method;Entropy;PSNR;Image segmentation;Accuracy;Licenses;Character recognition;Feature extraction;thresholding;segmentation;OCR;peak signal to noise ratio},
}

@InProceedings{PirahanSiah2010,
  author    = {F. {PirahanSiah} and S. N. H. S. {Abdullah} and S. {Sahran}},
  booktitle = {2010 International Conference on Computer Applications and Industrial Electronics},
  title     = {Adaptive image segmentation based on peak signal-to-noise ratio for a license plate recognition system},
  year      = {2010},
  month     = {Dec},
  pages     = {468-472},
  abstract  = {The objective of this paper is to propose an adaptive threshold method based on peak signal to noise ratio (PSNR). Nowadays, PSNR has been widely used as stopping criteria in multi level threshold method for segmenting images. Alternatively, we apply the PSNR as criteria to find the most suitable threshold value. We evaluate this proposed method on license plate recognition application. At the same time, we compare this proposed algorithm with multi-level and multi-threshold methods as the benchmark. Via the proposed technique, it could relatively change according to environment such as when there is a high or low contrast situation.},
  doi       = {10.1109/ICCAIE.2010.5735125},
  file      = {:farshid/conference/Adaptive image segmentation based on Peak Signalto-Noise Ratio for a license plate recognition system_2010.pdf:PDF},
  groups    = {Farshid PirahanSiah},
  keywords  = {image recognition;image segmentation;traffic engineering computing;adaptive image segmentation;peak signal-to-noise ratio;license plate recognition system;adaptive multilevel threshold method;Licenses;Image segmentation;PSNR;Image recognition;Optical character recognition software;Pattern recognition;Accuracy;Adaptive threshold;image segmentation;license plate recognition;thresholding image segmentation;PSNR},
}

@Book{PirahanSiah2017a,
  author    = {Farshid PirahanSiah, S.N.H.S. Abdullah, Shahnorbanun Sahran},
  editor    = {Siti Norul Huda Sheikh AbdullahSeyed Mostafa Mousavi Kahaki Akmal Aris},
  publisher = {Penerbit Universiti Kebangsaan Malaysia},
  title     = {CHAPTER 4: Augmented Optical Flow Methods for Video Stabilization Computational Intelligence: from theory to application},
  year      = {2017},
  isbn      = {9789674124588},
  file      = {:farshid/book/farshid_book.pdf:PDF},
  groups    = {Farshid PirahanSiah},
}

@InProceedings{Asgari2011,
  author       = {Asgari, Maryam and Shahverdy, Mohammad and Pirahansiah, Farshid and Mahdavi, Zahra},
  booktitle    = {2011 IEEE 3rd International Conference on Communication Software and Networks},
  title        = {TafreshGrid: Grid computing in Tafresh university},
  year         = {2011},
  organization = {IEEE},
  pages        = {83--85},
  file         = {:farshid/conference/TafreshGrid Grid computing in Tafresh university.pdf:PDF},
  groups       = {Farshid PirahanSiah},
}

@InProceedings{PirahanSiah2012,
  author    = {PirahanSiah, Farshid and Abdullah, SNHS and Sahran, S},
  booktitle = {2nd National Doctoral Seminar on Artificial Intelligence Technology, Residence Hotel, UNITEN, Malaysia},
  title     = {2D versus 3D map for environment movement object},
  year      = {2012},
  file      = {:farshid/conference/2D versus 3D map for environment movement object_2012.pdf:PDF},
  groups    = {Farshid PirahanSiah},
}

@Article{Chen2020,
  author      = {Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey Hinton},
  title       = {A Simple Framework for Contrastive Learning of Visual Representations},
  abstract    = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  date        = {2020-02-13},
  eprint      = {2002.05709v2},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:pdf/SimCLR_A Simple Framework for Contrastive Learning of Visual Representations.pdf:PDF},
  groups      = {DRL},
  keywords    = {cs.LG, cs.CV, stat.ML},
  priority    = {prio3},
  readstatus  = {read},
}

@Article{Liu2017,
  author   = {Liu, Hao and Lu, Jiwen and Feng, Jianjiang and Zhou, Jie},
  journal  = {IEEE Transactions on Image Processing},
  title    = {{Learning Deep Sharable and Structural Detectors for Face Alignment}},
  year     = {2017},
  issn     = {1057-7149},
  number   = {4},
  pages    = {1--1},
  volume   = {26},
  abstract = {Face alignment aims at localizing multiple facial landmarks for a given facial image, which usually suffers from large variances of diverse facial expressions, aspect ratios and partial occlusions, especially when face images were captured in wild conditions. Conventional face alignment methods extract local features and then directly concatenate these features for global shape regression. Unlike these methods which cannot explicitly model the correlation of neighbouring landmarks and motivated by the fact that individual landmarks are usually correlated, we propose a deep sharable and structural detectors (DSSD) method for face alignment. To achieve this, we firstly develop a structural feature learning method to explicitly exploit the correlation of neighbouring landmarks, which learns to cover semantic information to disambiguate the neighbouring landmarks. Moreover, our model selectively learns a subset of sharable latent tasks across neighbouring landmarks under the paradigm of the multi-task learning framework, so that the redundancy information of the overlapped patches can be efficiently removed. To better improve the performance, we extend our DSSD to a recurrent DSSD (R-DSSD) architecture by integrating with the complementary information from multi-scale perspectives. Experimental results on the widely used benchmark datasets show that our methods achieve very competitive performance compared to the state-of-the-arts.},
  doi      = {10.1109/TIP.2017.2657118},
  keywords = {face recognition;feature extraction;learning (arti},
  url      = {http://ieeexplore.ieee.org/document/7829264/},
}

@Patent{Mnih2017,
  year      = {2017},
  author    = {Mnih, Volodymyr and Kavukcuoglu, Koray},
  month     = jun #{~13},
  note      = {US Patent 9,679,258},
  title     = {Methods and apparatus for reinforcement learning},
  file      = {:pdf/Patents/Methods and apparatus for reinforcement learning_2015.pdf:PDF},
  groups    = {Patent},
  publisher = {Google Patents},
}

@Patent{Ghesu2018,
  year      = {2018},
  author    = {Ghesu, Florin Cristian and Georgescu, Bogdan and Comaniciu, Dorin},
  month     = jul #{~24},
  note      = {US Patent 10,032,281},
  title     = {Multi-scale deep reinforcement machine learning for N-dimensional segmentation in medical imaging},
  file      = {:pdf/Patents/Multi-scale deep reinforcement machine learning for N-dimensional segmentation in medical imaging_2018.pdf:PDF},
  groups    = {Patent},
  publisher = {Google Patents},
}

@Misc{Nguyen2020,
  author        = {Ngoc Duy Nguyen and Thanh Thi Nguyen and Hai Nguyen and Saeid Nahavandi},
  title         = {Review, Analyze, and Design a Comprehensive Deep Reinforcement Learning Framework},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {2002.11883},
  file          = {:pdf/Review, Analyze, and Design a Comprehensive Deep Reinforcement Learning Framework.pdf:PDF},
  groups        = {DRL},
  primaryclass  = {cs.LG},
}

@Article{FrancoisLavet2018,
  author    = {François-Lavet, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G. and Pineau, Joelle},
  journal   = {Foundations and Trends® in Machine Learning},
  title     = {An Introduction to Deep Reinforcement Learning},
  year      = {2018},
  issn      = {1935-8245},
  number    = {3-4},
  pages     = {219–354},
  volume    = {11},
  doi       = {10.1561/2200000071},
  file      = {:pdf/IntroductionDRL.pdf:PDF},
  groups    = {DRL},
  publisher = {Now Publishers},
  url       = {http://dx.doi.org/10.1561/2200000071},
}

@Misc{Nagaraja2017,
  author    = {Nagaraja, Nagendra},
  month     = sep #{~5},
  note      = {US Patent 9,754,221},
  title     = {Processor for implementing reinforcement learning operations},
  year      = {2017},
  file      = {:pdf/Patents/Processor for implementing reinforcement learning operations .pdf:PDF},
  groups    = {Patent},
  publisher = {Google Patents},
}

@Article{Kaelbling1996,
  author  = {Kaelbling, Leslie Pack and Littman, Michael L and Moore, Andrew W},
  journal = {Journal of artificial intelligence research},
  title   = {Reinforcement learning: A survey},
  year    = {1996},
  pages   = {237--285},
  volume  = {4},
  file    = {:pdf/Reinforcement learning A survey_8Kcite.pdf:PDF},
  groups  = {DRL},
}

@InProceedings{Lange2010,
  author       = {Lange, Sascha and Riedmiller, Martin},
  booktitle    = {The 2010 International Joint Conference on Neural Networks (IJCNN)},
  title        = {Deep auto-encoder neural networks in reinforcement learning},
  year         = {2010},
  organization = {IEEE},
  pages        = {1--8},
  file         = {:pdf/Deep Auto-Encoder Neural Networks in Reinforcement Learning.pdf:PDF},
  groups       = {DRL},
}

@Article{Zhou2018,
  author  = {Zhou, Kaiyang and Xiang, Tao and Cavallaro, Andrea},
  journal = {arXiv preprint arXiv:1807.03089},
  title   = {Video summarisation by classification with deep reinforcement learning},
  year    = {2018},
  file    = {:pdf/Video Summarisation by Classification with Deep Reinforcement Learning.pdf:PDF},
  groups  = {DRL},
}

@Article{Gruslys2017,
  author  = {Gruslys, Audrunas and Dabney, Will and Azar, Mohammad Gheshlaghi and Piot, Bilal and Bellemare, Marc and Munos, Remi},
  journal = {arXiv preprint arXiv:1704.04651},
  title   = {The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning},
  year    = {2017},
  file    = {:pdf/the reactor a fast and sample-efficient actor-critic agent for reinforcement learning.pdf:PDF},
  groups  = {DRL},
}

@Article{Machado2018,
  author  = {Machado, Marlos C and Bellemare, Marc G and Talvitie, Erik and Veness, Joel and Hausknecht, Matthew and Bowling, Michael},
  journal = {Journal of Artificial Intelligence Research},
  title   = {Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents},
  year    = {2018},
  pages   = {523--562},
  volume  = {61},
  file    = {:pdf/Revisiting the Arcade Learning Environment Evaluation Protocols and Open Problems for General Agents.pdf:PDF},
  groups  = {DRL},
}

@InProceedings{Hessel2018,
  author    = {Hessel, Matteo and Modayil, Joseph and Van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  booktitle = {Thirty-Second AAAI Conference on Artificial Intelligence},
  title     = {Rainbow: Combining improvements in deep reinforcement learning},
  year      = {2018},
  file      = {:pdf/Rainbow Combining Improvements in Deep Reinforcement Learning.pdf:PDF},
  groups    = {DRL},
}

@Article{Salimans2017,
  author  = {Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya},
  journal = {arXiv preprint arXiv:1703.03864},
  title   = {Evolution strategies as a scalable alternative to reinforcement learning},
  year    = {2017},
  file    = {:pdf/Salimans2017 - Evolution Strategies As a Scalable Alternative to Reinforcement Learning.pdf:PDF},
  groups  = {DRL},
}

@InProceedings{Irpan2019,
  author    = {Irpan, Alexander and Rao, Kanishka and Bousmalis, Konstantinos and Harris, Chris and Ibarz, Julian and Levine, Sergey},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Off-policy evaluation via off-policy classification},
  year      = {2019},
  pages     = {5438--5449},
  file      = {:pdf/Off-Policy Evaluation via Off-Policy Classification.pdf:PDF},
  groups    = {DRL},
}

@Article{Bellman1957,
  author = {Bellman, RE},
  title  = {A markov decision process. journal of Mathematical Mechanics},
  year   = {1957},
  file   = {:pdf/a markovian decision process_1957.pdf:PDF},
  groups = {DRL},
}

@Article{Tian2019,
  author  = {Tian, Yuandong and Ma, Jerry and Gong, Qucheng and Sengupta, Shubho and Chen, Zhuoyuan and Pinkerton, James and Zitnick, C Lawrence},
  journal = {arXiv preprint arXiv:1902.04522},
  title   = {Elf opengo: An analysis and open reimplementation of alphazero},
  year    = {2019},
  file    = {:pdf/ELF OpenGo An Analysis and Open Reimplementation of AlphaZero.pdf:PDF},
  groups  = {DRL},
}

@Article{Tesauro1995,
  author  = {Tesauro, Gerald},
  journal = {Communications of the ACM},
  title   = {Temporal difference learning and TD-Gammon},
  year    = {1995},
  number  = {3},
  pages   = {58--68},
  volume  = {38},
  file    = {:pdf/Temporal Difference Learning and TD-Gammon.pdf:PDF},
  groups  = {DRL},
}

@Article{Le2018,
  author  = {Le, Hoang M and Jiang, Nan and Agarwal, Alekh and Dud{\'\i}k, Miroslav and Yue, Yisong and Daum{\'e} III, Hal},
  journal = {arXiv preprint arXiv:1803.00590},
  title   = {Hierarchical imitation and reinforcement learning},
  year    = {2018},
  file    = {:pdf/Hierarchical Imitation and Reinforcement Learning.pdf:PDF},
  groups  = {DRL},
}

@InProceedings{PirahanSiah2018,
  author    = {Farshid PirahanSiah and Hon Hock Woon Hon and Nik Ahmad Akram {Nik Zulkepeli}},
  booktitle = {2018 2nd Advanced Research in Electronic Engineering and Information Technology International Conference (AVAREIT'2018)},
  title     = {Effect of Augmented Synthetic Datasets to Train Convolutional neural networks},
  year      = {2018},
  address   = {Kuala Lumpur, Malaysia},
  month     = jan,
  abstract  = {The various number of images and large dataset to train deep neural
networks (DNNs) are key to accurate model. Dataset with poor translational
invariance and lack of information about orientation or more generally what
he calls {"}pose{"}. Pose information refers to 3D orientation relative to
the viewer but also lighting and color. Convolutional neural networks
(CNNs) are known to have trouble when objects are rotated or when lighting
conditions are changed. In this paper a method for augmented synthetic
dataset (ASD) for training CNNs is proposed. The utmost issue of utilizing
deep learning is its requirement of the large dataset. Moreover, in most of
the cases the existence dataset is not large enough to train deep learning
properly. In addition, labeling the data is time consuming. In this
research image enhancement, transformation and filtering are using for
increasing number of images in dataset. Large dataset is required in these
methods lead to have more accurate training in DNN. After improving the
dataset we will compare the accuracy of the new dataset with original one
with three different deep learning architecture such as, LeNet, AlexNet,
and GoogLeNet by using Caltech-101 and Caltech-256 dataset. Result show
that the multi augmentation method can increase the accuracy of CNNs by
increasing the number of images up to 80 percent in the training phase.},
  days      = {21},
  keywords  = {Deep Neural Networks; Convolutional neural networks; Computer Vision; Augmented Synthetic Dataset; Training GAN; multi augmentation methods},
  ranking   = {rank5},
}

@Patent{MOKAYED2021,
  number    = {WO2021107761A1},
  year      = {2021},
  yearfiled = {2019},
  author    = {Hamam MOKAYED;Hock Woon Hon;Den Fairol BIN SAMAON;Hasmarina BINTI HASAN;Farshid PIRAHANSIAH},
  title     = {A method for detecting a moving vehicle},
  groups    = {Farshid PirahanSiah},
}

@Patent{PIRAHANSIAH2021,
  number    = {WO2021060971A1},
  year      = {2021},
  yearfiled = {2019},
  author    = {Farshid PIRAHANSIAH;Hamam MOKAYED;Hock Woon Hon},
  title     = {A method for augmenting a plurality of face images},
  groups    = {Farshid PirahanSiah},
}

@Patent{PIRAHANSIAH2020,
  number    = {WO2020141969A3},
  year      = {2020},
  yearfiled = {2018},
  author    = {Farshid PIRAHANSIAH ; Hock Woon Hon ; Nik Ahmad Akram Bin NIK ZULKEPELI},
  title     = {System and method for providing advertisement contents based on facial analysis},
  groups    = {Farshid PirahanSiah},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Farshid PirahanSiah\;2\;1\;0x00ff00ff\;DH\;Farshid PirahanSiah My Papers\;;
1 StaticGroup:Markings\;2\;0\;\;\;\;;
1 StaticGroup:DRL\;0\;1\;0x0000ffff\;lightbulb-on\;Farshid PirahanSiah DRL Book\;;
2 StaticGroup:Patent\;0\;0\;0x664db3ff\;\;Patent DRL\;;
1 StaticGroup:Book\;0\;0\;0xcccc33ff\;\;\;;
}
