
@inproceedings{lowe_object_1999,
	title = {Object recognition from local scale-invariant features},
	volume = {2},
	isbn = {0-7695-0164-8},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=790410},
	doi = {10.1109/ICCV.1999.790410},
	abstract = {An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds},
	pages = {1150--1157},
	booktitle = {Proceedings of the Seventh {IEEE} International Conference on Computer Vision},
	publisher = {{IEEE}},
	author = {Lowe, David G.},
	date = {1999},
	pmid = {15806121},
	note = {{ISSN}: 0-7695-0164-8
Issue: [8
\_eprint: 0112017},
}

@inproceedings{montemerlo_fastslam_2002,
	title = {{FastSLAM}: A factored solution to the simultaneous localization and mapping problem},
	volume = {68},
	isbn = {0-262-51129-0},
	url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:FastSLAM+:+A+Factored+Solution+to+the+Simultaneous+Localization+and+Mapping+Problem{\#}0},
	doi = {10.1.1.16.2153},
	abstract = {The ability to simultaneously localize a robot and accurately map its surroundings is considered by many to be a key prerequisite of truly autonomous robots. However, few approaches to this problem scale up to handle the very large number of landmarks present in real environments. Kalman filter-based algorithms, for example, require time quadratic in the number of landmarks to incorporate each sensor observation. This paper presents {FastSLAM}, an algorithm that recursively estimates the full posterior distribution over robot pose and landmark locations, yet scales logarithmically with the number of landmarks in the map. This algorithm is based on a factorization of the posterior into a product of conditional landmark distributions and a distribution over robot paths. The algorithm has been run successfully on as many as 50,000 landmarks, environments far beyond the reach of previous approaches. Experimental results demonstrate the advantages and limitations of the {FastSLAM} algorithm on both simulated and real-world data.},
	pages = {593--598},
	booktitle = {Proc. of 8th National Conference on Artificial Intelligence/14th Conference on Innovative Applications of Artificial Intelligence},
	author = {Montemerlo, Michael and Thrun, Sebastian and Koller, Daphne and Wegbreit, Ben},
	date = {2002},
	note = {Issue: 2},
}

@book{ilboudo_facteurs_2016,
	edition = {Second},
	title = {Facteurs influen??ant le refus de consulter au centre de sant?? dans la r??gion rurale Ouest du Burkina Faso},
	volume = {28},
	isbn = {978-85-7811-079-6},
	url = {http://www.robots.ox.ac.uk/{~}vgg/hzbook/index.html},
	abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-α-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent {MM}- {GBSA} calculations. Using the best {RMSD} among the top 10 scoring poses as a metric, the success rate ({RMSD} ≤ 2.0 Å for the interface backbone atoms) increased from 21\% with default Glide {SP} settings to 58\% with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta {FlexPepDock} method (63\% success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40\% of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
	pagetotal = {391–397},
	number = {3},
	publisher = {Cambridge University Press, {ISBN}: 0521540518},
	author = {Ilboudo, Sidb??wendin David Olivier and Sombi??, Issa and Soubeiga, Andr?? Kamba and Dr??bel, Tania},
	date = {2016},
	doi = {10.1017/CBO9781107415324.004},
	pmid = {25246403},
	note = {{ISSN}: 09953914
Publication Title: Sante Publique
\_eprint: {arXiv}:1011.1669v3},
	keywords = {Burkina Faso, Delivery of health care, Motivation, Patient preference, Rural health services, Treatment refusal},
}

@article{pinto_why_2008,
	title = {Why is real-world visual object recognition hard?},
	volume = {4},
	issn = {1553734X},
	url = {http://dx.doi.org/10.1371{\%}2Fjournal.pcbi.0040027},
	doi = {10.1371/journal.pcbi.0040027},
	abstract = {Progress in understanding the brain mechanisms underlying vision requires the construction of computational models that not only emulate the brain's anatomy and physiology, but ultimately match its performance on visual tasks. In recent years, "natural" images have become popular in the study of vision and have been used to show apparently impressive progress in building such models. Here, we challenge the use of uncontrolled "natural" images in guiding that progress. In particular, we show that a simple V1-like model–a neuroscientist's "null" model, which should perform poorly at real-world visual object recognition tasks–outperforms state-of-the-art object recognition systems (biologically inspired and otherwise) on a standard, ostensibly natural image recognition test. As a counterpoint, we designed a "simpler" recognition test to better span the real-world variation in object pose, position, and scale, and we show that this test correctly exposes the inadequacy of the V1-like model. Taken together, these results demonstrate that tests based on uncontrolled natural images can be seriously misleading, potentially guiding progress in the wrong direction. Instead, we reexamine what it means for images to be natural and argue for a renewed focus on the core problem of object recognition–real-world image variation.},
	pages = {0151--0156},
	number = {1},
	journaltitle = {{PLoS} Computational Biology},
	author = {Pinto, Nicolas and Cox, David D. and {DiCarlo}, James J.},
	date = {2008},
	pmid = {18225950},
	note = {{ISBN}: 1553-7358
Publisher: Public Library of Science
\_eprint: {arXiv}:1202.2745v1},
}

@misc{koller_probabilistic_2009,
	title = {Probabilistic graphical models: principles and techniques},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.404.3622{\&}rep=rep1{\&}type=pdf{\%}5Cnhttp://books.google.com/books?hl=en{\&}lr={\&}id=7dzpHCHzNQ4C{\&}oi=fnd{\&}pg=PR9{\&}dq=Probabilistic+graphical+models+principles+and+techniques{\&}ots=pu7FAm4{\_}tQ{\&}sig=i3UYsiOk3mjRASsvTV78a8gPeP8},
	publisher = {{MIT} press},
	author = {Koller, D and Friedman, N},
	date = {2009},
	note = {Publication Title: Citeseer},
}

@inproceedings{abdullah_multi-threshold_2010,
	title = {Multi-threshold approach for license plate recognition system},
	rights = {All rights reserved},
	url = {http://www.waset.org/journals/waset/v72/v72-146.pdf},
	abstract = {The objective of this paper is to propose an adaptive multi threshold for image segmentation precisely in object detection. Due to the different types of license plates being used, the requirement of an automatic {LPR} is rather different for each country. The proposed technique is applied on Malaysian {LPR} application. It is based on Multi Layer Perceptron trained by back propagation. The proposed adaptive threshold is introduced to find the optimum threshold values. The technique relies on the peak value from the graph of the number object versus specific range of threshold values. That proposed approach has actually increased the overall performance compared to current optimal threshold techniques. Further improvement on this method is in progress to accommodate real time system specification.},
	pages = {1046--1050},
	booktitle = {International Conference on Signal and Image Processing {WASET} Singapore August 25-27, 2010 {ICSIP} 2010},
	author = {Abdullah, Siti Norul Huda Sheikh and {PirahanSiah}, Farshid and Zainal Abidin, Nur Hanisah and Sahran, Shahnorbanun},
	date = {2010},
}

@inproceedings{abdullah_evaluation_2010,
	title = {An evaluation of classification techniques using enhanced Geometrical Topological Feature Analysis},
	rights = {All rights reserved},
	abstract = {In this paper, we evaluate the best classification techniques for Malaysia license plate recognition ({LPR})system. We also discuss four image classification techniques that are used in contemporary {LPR} sys- tem worldwide. There are artificial immune recognition system, neural network, bayesian network and support vector machine. We propose and apply enhanced geometrical topological feature analysis on Malaysian character and number images as their inputs. We also explain character error analysis based on those image classification approaches. It shows that support vector machine outperforms compared to other classifiers.},
	pages = {12--22},
	booktitle = {2nd Malaysian Joint Conference on Artificial Intelligence ({MJCAI} 2010)},
	author = {Abdullah, Siti Norul Huda Sheikh and {PirahanSiah}, Farshid and Khalid, Marzuki and Omar, Khairuddin},
	date = {2010},
	keywords = {Computer Science, Farshid {PirahanSiah}, threshold},
}

@article{bengio_representation_2013,
	title = {Representation Learning: A Review and New Perspectives.},
	volume = {35},
	issn = {1939-3539},
	url = {http://arxiv.org/abs/1206.5538{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/23459267},
	doi = {3C2DBCEE-8A96-493B-B88B-36B1F52ECA58},
	abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for {AI} is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
	pages = {1--30},
	number = {1993},
	journaltitle = {Tpami},
	author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	date = {2013},
	pmid = {23459267},
	note = {{ISBN}: 0162-8828 {VO} - 35
\_eprint: {arXiv}:1206.5538v2},
	keywords = {cs.{LG}},
}

@article{pirahansiah_simultaneous_2013,
	title = {Simultaneous Localization and Mapping Trends and Humanoid Robot Linkages},
	volume = {2},
	rights = {All rights reserved},
	pages = {27--38},
	number = {2},
	journaltitle = {Asia-Pacific Journal of Information Technology and Multimedia},
	author = {Pirahansiah, Farshid and Norul, Siti and Sheikh, Huda and Sahran, Shahnorbanun},
	date = {2013},
	note = {Publisher: Penerbit Universiti Kebangsaan Malaysia},
	keywords = {3d vision, and mapping, cml, concurrent mapping and, humanoid robot, localisation, localization, stereo vision on slam, visual simultaneous localization, vslam},
	file = {Attachment:C\:\\Users\\farshidp\\Zotero\\storage\\5YDJRUY8\\Simultaneous Localization And Mapping Trends And Humanoid Robot Linkages_2013.pdf:application/pdf},
}

@article{pirahansiah_adaptive_2014,
	title = {Adaptive image thresholding based on the peak signal-to-noise Ratio},
	volume = {8},
	rights = {All rights reserved},
	issn = {20407467},
	pages = {1104--1116},
	number = {9},
	journaltitle = {Research Journal of Applied Sciences, Engineering and Technology},
	author = {{PirahanSiah}, Farshid and Abdullah, Siti Norul Huda Sheikh and Sahran, Shahnorbanun},
	date = {2014},
	note = {Publisher: \{\${\textbackslash}backslash\$copyright\} Maxwell Scientific Organization},
	keywords = {Image processing, Image segmentation, Optical character recognition, Single thresholding},
	file = {Attachment:C\:\\Users\\farshidp\\Zotero\\storage\\45FLGRPJ\\Adaptive Image Thresholding Based on the Peak Signal-to-noise Ratio_2014.pdf:application/pdf},
}

@article{ashtari_double_2015,
	title = {Double Line Image Rotation},
	volume = {24},
	issn = {1057-7149},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7117407},
	doi = {10.1109/TIP.2015.2440763},
	abstract = {This paper proposes a fast algorithm for rotating images while preserving their quality. The new approach rotates images based on vertical or horizontal lines in the original image and their rotated equation in the target image. The proposed method is a one-pass method that determines a based-line equation in the target image and extracts all corresponding pixels on the base-line. Floating-point multiplications are performed to calculate the base-line in the target image, and other line coordinates are calculated using integer addition or subtraction and logical justifications from the base-line pixel coordinates in the target image. To avoid a heterogeneous distance between rotated pixels in the target image, each line rotates to two adjacent lines. The proposed method yields good performance in terms of speed and quality according to the results of an analysis of the computation speed and accuracy.},
	pages = {3370--3385},
	number = {11},
	journaltitle = {{IEEE} Transactions on Image Processing},
	author = {Ashtari, Amir Hossein and Nordin, Md Jan and Kahaki, Seyed Mostafa Mousavi},
	date = {2015-11},
	keywords = {Image processing, Accuracy, Computer vision, {DLR}, Double-line rotation, image representation, image rotation, image transform, Interpolation, line rotation, Mathematical model, Shearing, Transforms},
}

@inproceedings{szegedy_going_2015,
	title = {Going deeper with convolutions},
	volume = {07-12-June-2015},
	isbn = {978-1-4673-6964-0},
	doi = {10.1109/CVPR.2015.7298594},
	abstract = {Abstract We propose a deep convolutional neural network architecture codenamed Incep- tion, which was responsible for setting the new state of the art for classification and detection in the {ImageNet} Large-Scale Visual Recognition Challenge 2014 ({ILSVRC}14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for {ILSVRC}14 is called {GoogLeNet}, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
	pages = {1--9},
	booktitle = {Proceedings of the {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	date = {2015},
	pmid = {24920543},
	note = {{ISSN}: 10636919
\_eprint: 1409.4842},
}

@article{russakovsky_imagenet_2015,
	title = {{ImageNet} Large Scale Visual Recognition Challenge},
	volume = {115},
	issn = {15731405},
	doi = {10.1007/s11263-015-0816-y},
	abstract = {The {ImageNet} Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide detailed a analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
	pages = {211--252},
	number = {3},
	journaltitle = {International Journal of Computer Vision},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	date = {2015},
	pmid = {16190471},
	note = {{ISBN}: 0920-5691
\_eprint: 1409.0575},
	keywords = {Benchmark, Dataset, Large-scale, Object detection, Object recognition},
}

@article{ghesu_marginal_2016,
	title = {Marginal Space Deep Learning: Efficient Architecture for Volumetric Image Parsing},
	volume = {35},
	issn = {1558254X},
	doi = {10.1109/TMI.2016.2538802},
	abstract = {Conference proceddings},
	pages = {1217--1228},
	number = {5},
	journaltitle = {{IEEE} Transactions on Medical Imaging},
	author = {Ghesu, Florin C. and Krubasik, Edward and Georgescu, Bogdan and Singh, Vivek and Zheng, Yefeng and Hornegger, Joachim and Comaniciu, Dorin},
	date = {2016-05},
	pmid = {23285570},
	note = {{ISBN}: 978-3-319-24552-2},
	keywords = {Deep learning, image parsing, marginal space learning, sparse representations, three-dimensional (3D) object detection and segmentation},
}

@article{pirahansiah_gsft-psnr_2016,
	title = {{GSFT}-{PSNR}: Global Single Fuzzy Threshold Based on {PSNR} for {OCR} Systems},
	volume = {4},
	rights = {All rights reserved},
	pages = {1--19},
	number = {6},
	journaltitle = {International Journal of Computer Science and Network Solutions},
	author = {{PirahanSiah}, Farshid and Shahverdy, Mohammad},
	date = {2016},
	note = {Publisher: International journal of Computer Science and Network Solutions ({IJCSNS})},
	file = {Attachment:C\:\\Users\\farshidp\\Zotero\\storage\\M6GE2MEC\\GSFT-PSNR Global Single Fuzzy Threshold_2016.pdf:application/pdf},
}

@article{jain_statistical_2000,
	title = {Statistical pattern recognition: A review},
	volume = {22},
	issn = {01628828},
	doi = {10.1109/34.824819},
	abstract = {The primary goal of pattern recognition is supervised or unsupervised classification. Among the various frameworks in which pattern recognition has been traditionally formulated, the statistical approach has been most intensively studied and used in practice. More recently, neural network techniques and methods imported from statistical learning theory have been receiving increasing attention. The design of a recognition system requires careful attention to the following issues: definition of pattern classes, sensing environment, pattern representation, feature extraction and selection, cluster analysis, classifier design and learning, selection of training and test samples, and performance evaluation. In spite of almost 50 years of research and development in this field, the general problem of recognizing complex patterns with arbitrary orientation, location, and scale remains unsolved. New and emerging applications, such as data mining, web searching, retrieval of multimedia data, face recognition, and cursive handwriting recognition, require robust and efficient pattern recognition techniques. The objective of this review paper is to summarize and compare some of the well-known methods used in various stages of a pattern recognition system and identify research topics and applications which are at the forefront of this exciting and challenging field.},
	pages = {4--37},
	number = {1},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Jain, Anil K. and Duin, Robert P W and Mao, Jianchang},
	date = {2000},
	pmid = {17542025},
	note = {{ISBN}: 0162-8828 {VO} - 22
\_eprint: {arXiv}:1011.1669v3},
}

@article{perona_scale-space_1990,
	title = {Scale-Space and Edge Detection Using Anisotropic Diffusion},
	volume = {12},
	issn = {01628828},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=56205},
	doi = {10.1109/34.56205},
	abstract = {A new definition of scale-space is suggested, and a class of algorithms used to realize a diffusion process is introduced. The diffusion coefficient is chosen to vary spatially in such a way as to encourage intraregion smoothing rather than interregion smoothing. It is shown that the 'no new maxima should be generated at coarse scales' property of conventional scale space is preserved. As the region boundaries in our approach remain sharp, a high-quality edge detector which successfully exploits global information is obtained. Experimental results are shown on a number of images. Parallel hardware implementations are made feasible because the algorithm involves elementary, local operations replicated over the image.},
	pages = {629--639},
	number = {7},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Perona, Pietro and Malik, Jitendra},
	date = {1990},
	pmid = {19268610},
	note = {{ISBN}: 0162-8828},
	keywords = {Adaptive filtering, Analog {VLSI}, Edge detection, Edge enhancement, Nonlinear diffusion, Nonlinear filtering, Parallel algorithm, Scale-space},
}

@article{scharstein_taxonomy_2002,
	title = {A Taxonomy and Evaluation of Dense Two-Frame Stereo Correspondence Algorithms},
	volume = {47},
	issn = {1573-1405},
	url = {http://dx.doi.org/10.1023/A:1014573219977},
	doi = {10.1023/A:1014573219977},
	abstract = {Stereo matching is one of the most active research areas in computer vision. While a large number of algorithms for stereo correspondence have been developed, relatively little work has been done on characterizing their performance. In this paper, we present a taxonomy of dense, two-frame stereo methods. Our taxonomy is designed to assess the different components and design decisions made in individual stereo algorithms. Using this taxonomy, we compare existing stereo methods and present experiments evaluating the performance of many different variants. In order to establish a common software platform and a collection of data sets for easy evaluation, we have designed a stand-alone, flexible C++ implementation that enables the evaluation of individual components and that can easily be extended to include new algorithms. We have also produced several new multi-frame stereo data sets with ground truth and are making both the code and data sets available on the Web. Finally, we include a comparative evaluation of a large set of today's best-performing stereo algorithms.},
	pages = {7--42},
	number = {1},
	journaltitle = {International Journal of Computer Vision},
	author = {Scharstein, Daniel and Szeliski, Richard},
	date = {2002},
	pmid = {350},
	note = {{ISBN}: 0-7695-1327-1},
}

@inproceedings{oertel_current_2008,
	location = {Washington, {DC}, {USA}},
	title = {Current challenges in automating visual perception},
	isbn = {978-1-4244-3125-0},
	url = {http://dx.doi.org/10.1109/AIPR.2008.4906457},
	doi = {10.1109/AIPR.2008.4906457},
	abstract = {After nearly half a century of computer vision research, application-specific systems are common but the goal of developing a robust, general-purpose computer vision system remains out of reach. Rather than focus on the strengths and weaknesses of current computer vision approaches, this paper will enumerate and investigate the challenges that must be overcome before this goal can be achieved. Key challenges include handling variations in environment or acquisition parameters such as lighting, view angle, distance, and image quality; recognizing naturally occurring as well as intentionally deceptive variations in object appearance; providing robust general-purpose image segmentation and co-registration; generating 3D representations from 2D images; developing useful object representations; providing required knowledge that is not represented in the image itself; and managing computational complexity. Each of these challenges, along with their relevance to solving the vision problem, will be discussed. Understanding these challenges as a whole may provide insight into underlying mechanisms that will provide the backbone of a robust general-purpose computer vision system.},
	pages = {1--8},
	booktitle = {Proceedings - Applied Imagery Pattern Recognition Workshop},
	publisher = {{IEEE} Computer Society},
	author = {Oertel, Carsten and Colder, Brian and Colombe, Jeffrey and High, Julia and Ingram, Michael and Sallee, Phil},
	date = {2008},
	note = {{ISSN}: 15505219},
}

@inproceedings{elgammal_non-parametric_2000,
	title = {Non-parametric model for background subtraction},
	volume = {1843},
	isbn = {3-540-67686-4},
	url = {http://www.springerlink.com/index/3mcvhnwfa8bj4ln5.pdf{\%}5Cnhttp://link.springer.com/chapter/10.1007/3-540-45053-X{\_}48},
	doi = {10.1007/3-540-45053},
	abstract = {Background subtraction is a method typically used to segment moving regions in image sequences taken from a static camera by comparing each new frame to a model of the scene background. We present a novel non-parametric background model and a background subtraction approach. The model can handle situations where the background of the scene is cluttered and not completely static but contains small motions such as tree branches and bushes. The model estimates the probability of observing pixel intensity values based on a sample of intensity values for each pixel. The model adapts quickly to changes in the scene which enables very sensitive detection of moving targets. We also show how the model can use color information to suppress detection of shadows. The implementation of the model runs in real-time for both gray level and color imagery. Evaluation shows that this approach achieves very sensitive detection with very low false alarm rates.},
	pages = {751--767},
	booktitle = {Computer Vision—{ECCV} 2000},
	author = {Elgammal, Ahmed and Harwood, David and Davis, Larry},
	date = {2000},
	note = {{ISSN}: 16113349},
	keywords = {active real time, motion detection, non parametric estimation, shadow detection, vision, visual motion, visual surveillance},
}

@article{mikolajczyk_performance_2005,
	title = {A performance evaluation of local descriptors},
	volume = {27},
	issn = {01628828},
	doi = {10.1109/TPAMI.2005.188},
	abstract = {In this paper, we compare the performance of descriptors computed for local interest regions, as, for example, extracted by the Harris-Affine detector [32]. Many different descriptors have been proposed in the literature. It is unclear which descriptors},
	pages = {1615--1630},
	number = {10},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Mikolajczyk, Krystian and Schmid, Cordelia},
	date = {2005},
	pmid = {16237996},
	note = {{ISBN}: 0769519008},
	keywords = {Interest points, Interest regions, Invariance, Local descriptors, Matching, Recognition},
}

@inproceedings{sivic_video_2003,
	title = {Video Google: a text retrieval approach to object matching in videos},
	volume = {2},
	isbn = {0-7695-1950-4},
	url = {http://ieeexplore.ieee.org/xpl/login.jsp?tp={\&}arnumber=1238663},
	doi = {10.1109/ICCV.2003.1238663},
	abstract = {We describe an approach to object and scene retrieval which searches for and localizes all the occurrences of a user outlined object in a video. The object is represented by a set of viewpoint invariant region descriptors so that recognition can proceed successfully despite changes in viewpoint, illumination and partial occlusion. The temporal continuity of the video within a shot is used to track the regions in order to reject unstable regions and reduce the effects of noise in the descriptors. The analogy with text retrieval is in the implementation where matches on descriptors are pre-computed (using vector quantization), and inverted file systems and document rankings are used. The result is that retrieved is immediate, returning a ranked list of key frames/shots in the manner of Google. The method is illustrated for matching in two full length feature films.},
	pages = {1470--1477},
	booktitle = {Proceedings of the International Conference on Computer Vision},
	author = {Sivic, Josef and Zisserman, Andrew},
	date = {2003},
	pmid = {25052830},
	note = {{ISSN}: 00189219
Issue: Iccv
\_eprint: 1504.06897},
}

@article{kwak_image_2014,
	title = {Image contrast enhancement for intelligent surveillance systems using multi-local histogram transformation},
	volume = {25},
	issn = {15728145},
	url = {http://dx.doi.org/10.1007/s10845-012-0663-4},
	doi = {10.1007/s10845-012-0663-4},
	abstract = {Among all applications to monitor the safety and security of working environments, surveillance systems that use computer vision are the most efficient and intuitive in the manufacturing industry. This paper introduces a new technique of contrast enhancement for surveillance systems using computer vision. The histogram equalization method is a common and widespread image enhancement method which maximizes the contrast of the image. This contrast enhancement method usually improves the quality of images, but it can suffer from visual deterioration caused by excessive histogram modification. To overcome the limitations of conventional contrast enhancement methods, this paper introduces a new multi-local histogram transformation method for surveillance systems. This technique is based on the local histograms, which are separated from the overall histogram of the image, and the contrast of the image can be enhanced through two major processes: range reassignment of local histograms and local histogram equalization. The multi-local histogram transformation in this paper enhances the contrast of images, preventing excessive compression and extension of image histograms. The performance of the suggested contrast enhancement method is verified by the experiments in four different environments.},
	pages = {303--318},
	number = {2},
	journaltitle = {Journal of Intelligent Manufacturing},
	author = {Kwak, Hwan Joo and Park, Gwi Tae},
	date = {2014},
	note = {{ISBN}: 0956-5515},
	keywords = {Histogram equalization ({HE}), Image enhancement, Multi-local histogram transformation ({MLHT}), Surveillance system},
}

@article{rowley_neural_1998,
	title = {Neural network-based face detection},
	volume = {20},
	issn = {01628828},
	doi = {10.1109/34.655647},
	abstract = {idsia},
	pages = {23--38},
	number = {1},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Rowley, Henry A. and Baluja, Shumeet and Kanade, Takeo},
	date = {1998},
	note = {{ISBN}: 0-8186-8497-6},
	keywords = {Computer vision, Artificial neural networks, Face detection, Machine learning, pattern recognition},
}

@article{smeulders_content_2000,
	title = {Content based image retrieval at the end of the early years},
	volume = {22(12)},
	url = {http://ieeexplore.ieee.org/xpl/freeabs{\_}all.jsp?tp={\&}arnumber=895972{\&}isnumber=19391},
	pages = {1349--1380},
	number = {12},
	journaltitle = {{IEEE} Trans. On Pattern Analysis and Machine Intelligence},
	author = {Smeulders, A and Worring, M and Santini, S and Gupta, A and Jain, R},
	date = {2000},
}

@inproceedings{cheung_hybrid_2011,
	title = {Hybrid evolution of convolutional networks},
	volume = {1},
	isbn = {978-0-7695-4607-0},
	doi = {10.1109/ICMLA.2011.73},
	abstract = {With the increasing trend of neural network models towards larger structures with more layers, we expect a corresponding exponential increase in the number of possible architectures. In this paper, we apply a hybrid evolutionary search procedure to define the initialization and architectural parameters of convolutional networks, one of the first successful deep network models. We make use of stochastic diagonal Levenberg-Marquardt to accelerate the convergence of training, lowering the time cost of fitness evaluation. Using parameters found from the evolutionary search together with absolute value and local contrast normalization preprocessing between layers, we achieve the best known performance on several of the {MNIST} Variations, rectangles-image and convex image datasets.},
	pages = {293--297},
	booktitle = {Proceedings - 10th International Conference on Machine Learning and Applications, {ICMLA} 2011},
	author = {Cheung, Brian and Sable, Carl},
	date = {2011-12},
	keywords = {convolutional networks, evolution, image classification, neural networks, second order methods},
}

@article{xia_learning_2016,
	title = {Learning rich features from objectness estimation for human lying-pose detection},
	issn = {09424962},
	url = {http://dx.doi.org/10.1007/s00530-016-0518-5},
	doi = {10.1007/s00530-016-0518-5},
	abstract = {Multimedia Systems, doi:10.1007/s00530-016-0518-5},
	pages = {1--12},
	journaltitle = {Multimedia Systems},
	author = {Xia, Dao Xun and Su, Song Zhi and Geng, Li Chuan and Wu, Guo Xi and Li, Shao Zi},
	date = {2016},
	note = {{ISBN}: 0053001605},
	keywords = {Deep model, Human lying-pose detection, Objectness estimation, Rich features learning, Saliency detection},
}

@article{caselles_geodesic_1997,
	title = {Geodesic Active Contours},
	volume = {22},
	url = {ftp://ftp-sop.inria.fr/athena/Team/Rachid.Deriche/Robotvis/Draft/GAC{\_}article.pdf},
	pages = {61--79},
	number = {1},
	journaltitle = {Ijcv},
	author = {Caselles, V and Kimmel, R and Sapiro, G},
	date = {1997},
}

@article{krasin_openimages_2016,
	title = {{OpenImages}: A public dataset for large-scale multi-label and multi-class image classification.},
	journaltitle = {Dataset available from https://github.com/openimages},
	author = {Krasin, Ivan and Duerig, Tom and Alldrin, Neil and Veit, Andreas and Abu-El-Haija, Sami and Belongie, Serge and Cai, David and Feng, Zheyun and Ferrari, Vittorio and Gomes, Victor and Gupta, Abhinav and Narayanan, Dhyanesh and Sun, Chen and Chechik, Gal and Murphy, Kevin},
	date = {2016},
}

@inproceedings{yoshioka_noise_2016,
	title = {Noise robust speech recognition using recent developments in neural networks for computer vision},
	isbn = {{VO} -},
	doi = {10.1109/ICASSP.2016.7472775},
	abstract = {Convolutional Neural Networks ({CNNs}) are superior to fully connected neural networks in various speech recognition tasks and the advantage is pronounced in noisy environments. In recent years, many techniques have been proposed in the computer vision community to improve {CNN}'s classification performance. This paper considers two approaches recently developed for image classification and examines their impacts on noisy speech recognition performance. The first approach is to increase the depth of convolution layers. Different approaches to deepening the {CNNs} are compared. In particular, the usefulness of learning dynamic features with small convolution layers that perform convolution in time is shown along with a modulation frequency analysis of the learned convolution filters. The second approach is to use trainable activation functions. Specifically, the use of a Parametric Rectified Linear Unit ({PReLU}) is investigated. Experimental results show that both approaches yield significant improvements in performance. Combining the two approaches further reduces recognition errors, producing a word error rate of 11.1\% in the Aurora4 task, the best published result for this corpus, with a standard one-pass bi-gram decoding set-up.},
	pages = {5730--5734},
	booktitle = {2016 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	author = {Yoshioka, T and Ohnishi, K and Fang, F and Nakatani, T},
	date = {2016},
	note = {{ISSN}: 15206149},
	keywords = {image classification, Acoustics, Automatic speech recognition, {CNN}, computer vision, Convolution, convolution filters, convolutional neural network, convolutional neural networks, Hidden Markov models, learning dynamic features, modulation frequency analysis, neural nets, Neural networks, Noise measurement, noise robust speech recognition, noise robustness, parametric rectified linear unit, {PReLU}, speech recognition, Speech recognition, Training},
}

@article{lecun_backpropagation_1989,
	title = {Backpropagation Applied to Handwritten Zip Code Recognition},
	volume = {1},
	issn = {0899-7667},
	doi = {10.1162/neco.1989.1.4.541},
	abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the {US} Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
	pages = {541--551},
	number = {4},
	journaltitle = {Neural Computation},
	author = {{LeCun}, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
	date = {1989},
	pmid = {1000111957},
	note = {{ISBN}: 0899-7667},
}

@article{freeman_example-based_2002,
	title = {Example-Based Super-Resolution},
	volume = {22},
	pages = {56--65},
	issue = {April},
	journaltitle = {{IEEE} Computer graphics and Applications},
	author = {Freeman, William T and Jones, Thouis R and Pasztor, Egon C},
	date = {2002},
}

@book{duda_pattern_2001,
	title = {Pattern Classification},
	publisher = {John Wiley \& Sons},
	author = {Duda, Richard O and Hart, Perter E and Stork, David G},
	date = {2001},
}

@article{sinha_new_2014,
	title = {New color {GPHOG} descriptors for object and scene image classification},
	volume = {25},
	issn = {09328092},
	url = {http://dx.doi.org/10.1007/s00138-013-0561-6},
	doi = {10.1007/s00138-013-0561-6},
	abstract = {This paper presents a novel set of image descriptors that encodes information from color, shape, spatial and local features of an image to improve upon the popular Pyramid of Histograms of Oriented Gradients ({PHOG}) descriptor for object and scene image classification. In particular, a new Gabor-{PHOG} ({GPHOG}) image descriptor created by enhancing the local features of an image using multiple Gabor filters is first introduced for feature extraction. Second, a comparative assessment of the classification performance of the {GPHOG} descriptor is made in grayscale and six different color spaces to further propose two novel color {GPHOG} descriptors that perform well on different object and scene image categories. Finally, an innovative Fused Color {GPHOG} ({FC}–{GPHOG}) descriptor is presented by integrating the Principal Component Analysis ({PCA}) features of the {GPHOG} descriptors in the six color spaces to combine color, shape and local feature information. Feature extraction for the proposed descriptors employs {PCA} and Enhanced Fisher Model ({EFM}), and the nearest neighbor rule is used for final classification. Experimental results using the {MIT} Scene dataset and the Caltech 256 object categories dataset show that the proposed new {FC}–{GPHOG} descriptor achieves a classification performance better than or comparable to other popular image descriptors, such as the Scale Invariant Feature Transform ({SIFT}) based Pyramid Histograms of visual Words descriptor, Color {SIFT} four Concentric Circles, Spatial Envelope, and Local Binary Patterns.},
	pages = {361--375},
	number = {2},
	journaltitle = {Machine Vision and Applications},
	author = {Sinha, Atreyee and Banerji, Sugata and Liu, Chengjun},
	date = {2014},
	keywords = {Color image search, {FC}-{GPHOG}, Gabor-{PHOG} ({GPHOG}), {YCbCr}-{GPHOG}, {YIQ}-{GPHOG}},
}

@article{cheng_learning_2016,
	title = {Learning Rotation-Invariant Convolutional Neural Networks for Object Detection in {VHR} Optical Remote Sensing Images},
	volume = {{PP}},
	issn = {01962892},
	doi = {10.1109/TGRS.2016.2601622},
	abstract = {Object detection in very high resolution optical remote sensing images is a fundamental problem faced for remote sensing image analysis. Due to the advances of powerful feature representations, machine-learning-based object detection is receiving increasing attention. Although numerous feature representations exist, most of them are handcrafted or shallow-learning-based features. As the object detection task becomes more challenging, their description capability becomes limited or even impoverished. More recently, deep learning algorithms, especially convolutional neural networks ({CNNs}), have shown their much stronger feature representation power in computer vision. Despite the progress made in nature scene images, it is problematic to directly use the {CNN} feature for object detection in optical remote sensing images because it is difficult to effectively deal with the problem of object rotation variations. To address this problem, this paper proposes a novel and effective approach to learn a rotation-invariant {CNN} ({RICNN}) model for advancing the performance of object detection, which is achieved by introducing and learning a new rotation-invariant layer on the basis of the existing {CNN} architectures. However, different from the training of traditional {CNN} models that only optimizes the multinomial logistic regression objective, our {RICNN} model is trained by optimizing a new objective function via imposing a regularization constraint, which explicitly enforces the feature representations of the training samples before and after rotating to be mapped close to each other, hence achieving rotation invariance. To facilitate training, we first train the rotation-invariant layer and then domain-specifically fine-tune the whole {RICNN} network to further boost the performance. Comprehensive evaluations on a publicly available ten-class object detection data set demonstrate the effectiveness of the proposed method.},
	pages = {1--11},
	number = {99},
	journaltitle = {{IEEE} Transactions on Geoscience and Remote Sensing},
	author = {Cheng, Gong and Zhou, Peicheng and Han, Junwei},
	date = {2016-12},
	note = {{ISBN}: 9781509014798},
	keywords = {Convolutional neural networks ({CNNs}), machine learning, object detection, remote sensing images, rotation-invariant {CNN} ({RICNN})},
}

@misc{pirahansiah_farshid_2020,
	title = {Farshid {PirahanSiah} Website},
	rights = {All rights reserved},
	url = {http://www.pirahansiah.com/},
	author = {{PirahanSiah}, Farshid},
	editor = {{PirahanSiah}, Farshid},
	date = {2020-04},
}

@book{kaehler_learning_2015,
	title = {Learning {OpenCV} 3: Computer Vision in C++ with the {OpenCV} Library},
	isbn = {1-4493-1465-1 978-1-4493-1465-1},
	pagetotal = {650},
	publisher = {" {OŔeilly} Media, Inc."},
	author = {Kaehler, Adrian and Bradski, Gary},
	date = {2015},
}

@article{griffin_caltech-256_2007,
	title = {Caltech-256 object category dataset},
	volume = {11},
	url = {http://authors.library.caltech.edu/7694},
	abstract = {We introduce a challenging set of 256 object categories containing a total of 30607 images. The original Caltech-101 1 was collected by choosing a set of object categories, downloading examples from Google Images and then manually screening out all images that did not fit the category. Caltech-256 is collected in a similar manner with several improvements: a) the number of categories is more than doubled, b) the minimum number of images in any category is increased from 31 to 80, c) aftifacts due to image rotation are avoided and d) a new and larger clutter category is introduced for testing background rejection. We suggest several testing paradigms to measure classification performance, the benchmark the dataset using two simple metrics as well as a state-of-the-art spatial pyramid matching 2 algorithm. Finally we use the clutter category to train an interest detector which rejects uninformative background regions.},
	pages = {20},
	number = {1},
	journaltitle = {Caltech mimeo},
	author = {Griffin, G and Holub, a and Perona, P},
	date = {2007},
	note = {{ISBN}: {UCB}/{CSD}-04-1366},
}

@inproceedings{pirahansiah_comparison_2011,
	title = {Comparison single thresholding method for handwritten images segmentation},
	rights = {All rights reserved},
	booktitle = {International Conference on Pattern Analysis and Intelligent Robotics},
	author = {{PirahanSiah}, Farshid and Abdullah, Siti Norul Huda Sheikh and Sahran, Shahnorbanun},
	date = {2011},
}

@inproceedings{viola_rapid_2001,
	title = {Rapid object detection using a boosted cascade of simple features},
	volume = {1},
	isbn = {0-7695-1272-0},
	doi = {10.1109/CVPR.2001.990517},
	abstract = {This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the "integral image" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on {AdaBoost}, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a "cascade" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.},
	pages = {I--511--I--518},
	booktitle = {Computer Vision and Pattern Recognition ({CVPR})},
	author = {Viola, P and Jones, M},
	date = {2001},
	pmid = {7143246},
	note = {{ISSN}: 1063-6919
\_eprint: {arXiv}:1011.1669v3},
	keywords = {image representation, Object detection, Face detection, Machine learning, image classification, machine learning, object detection, {AdaBoost}, background regions, boosted simple feature cascade, classifiers, Detectors, face detection, feature extraction, Filters, Focusing, image processing, Image representation, integral image, learning (artificial intelligence), object specific focus-of-attention mechanism, Pixel, rapid object detection, real-time applications, Robustness, Skin, statistical guarantees, visual object detection},
}

@inproceedings{kenny_time-frequency_1997,
	title = {Time-frequency methods for enhancing speech},
	volume = {3162},
	doi = {10.1117/12.284192},
	pages = {48--57},
	booktitle = {Optical Science, Engineering and Instrumentation9́7},
	author = {Kenny, Owen P and Nelson, Douglas J and Meade, Fort George G},
	date = {1997},
	note = {{ISSN}: 0277786X},
	keywords = {hermite polynomials, image enhancement, prolate-spheroidal fil-, singular value decomposition, speech enhancement, ter, time-frequency distribution, wiener filtering},
}

@article{davtalab_multi-level_2014,
	title = {Multi-Level Fuzzy Min-Max Neural Network Classifier},
	volume = {25},
	abstract = {In this paper a multi-level fuzzy min-max neural network classifier ({MLF}), which is a supervised learning method, is described. {MLF} uses basic concepts of the fuzzy min-max ({FMM}) method in a multi-level structure to classify patterns. This method uses separate classifiers with smaller hyperboxes in different levels to classify the samples that are located in overlapping regions. The final output of the network is formed by combining the outputs of these classifiers. {MLF} is capable of learning nonlinear boundaries with a single pass through the data. According to the obtained results, the {MLF} method, compared to the other {FMM} networks, has the highest performance and the lowest sensitivity to maximum size of the hyperbox parameter (\$þeta\$), with a training accuracy of 100\% in most cases.},
	pages = {470--482},
	number = {3},
	journaltitle = {{IEEE} Transactions on Neural Networks and Learning Systems},
	author = {Davtalab, R and Dezfoulian, M H and Mansoorizadeh, M},
	date = {2014},
	keywords = {fuzzy neural nets, learning (artificial intelligenc},
}

@article{krizhevsky_imagenet_2012,
	title = {{ImageNet} Classification with Deep Convolutional Neural Networks},
	volume = {22},
	issn = {10495258},
	doi = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the {ImageNet} {LSRVRC}-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient {GPU} implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the {ILSVRC}-2012 competition and achievd a top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	pages = {1--9},
	number = {1},
	journaltitle = {Advances In Neural Information Processing Systems},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {{PirahanSiah}, Farshid},
	date = {2012-12},
	pmid = {7491034},
	note = {{ISBN}: 9781627480031
Place: Washington, {DC}, {USA}
Publisher: {MIT} press
\_eprint: 1102.0183},
	keywords = {Computer Science, Farshid {PirahanSiah}, threshold, cs.{LG}, 3d vision, and mapping, cml, concurrent mapping and, humanoid robot, localisation, localization, stereo vision on slam, visual simultaneous localization, vslam, Image processing, Image segmentation, Optical character recognition, Single thresholding, Accuracy, Computer vision, {DLR}, Double-line rotation, image representation, image rotation, image transform, Interpolation, line rotation, Mathematical model, Shearing, Transforms, Object detection, Object recognition, Deep learning, image parsing, marginal space learning, sparse representations, active real time, motion detection, non parametric estimation, shadow detection, vision, visual motion, visual surveillance, Interest points, Histogram equalization ({HE}), Image enhancement, Multi-local histogram transformation ({MLHT}), Surveillance system, Face detection, Machine learning, pattern recognition, convolutional networks, evolution, image classification, neural networks, second order methods, Deep model, Human lying-pose detection, Objectness estimation, Rich features learning, Saliency detection, Acoustics, Automatic speech recognition, {CNN}, computer vision, Convolution, convolution filters, convolutional neural network, convolutional neural networks, Hidden Markov models, learning dynamic features, modulation frequency analysis, neural nets, Neural networks, Noise measurement, noise robust speech recognition, noise robustness, parametric rectified linear unit, {PReLU}, speech recognition, Speech recognition, Training, Color image search, {FC}-{GPHOG}, Gabor-{PHOG} ({GPHOG}), {YCbCr}-{GPHOG}, {YIQ}-{GPHOG}, machine learning, object detection, remote sensing images, {AdaBoost}, background regions, boosted simple feature cascade, classifiers, Detectors, face detection, feature extraction, Filters, Focusing, image processing, Image representation, integral image, learning (artificial intelligence), object specific focus-of-attention mechanism, Pixel, rapid object detection, real-time applications, Robustness, Skin, statistical guarantees, visual object detection, hermite polynomials, image enhancement, prolate-spheroidal fil-, singular value decomposition, speech enhancement, ter, time-frequency distribution, wiener filtering, fuzzy neural nets, 1, 23, 3D context, anatomical object detection, anatomical pose estimation, anisotropic diffusion, Anisotropic magnetoresistance, annotated image databases, aortic valve, automated feature design, Bayes-Verfahren, Benchmark testing, Biological neural networks, biomedical ultrasonics, biometrics, boundary delineation, Camera calibration, {CAR} (Roboter), character recognition, Classification, clinical workflow, clustered high-probability regions, comprehensive survey on object, computational efficiency, Computer architecture, Context, Contrast, convolutional neural network ({CNN}), Correlation, Decision support systems, deep learning, deep learning network architectures, deep learning systems, deep sharable and structural detectors method, detection in optical remote, diagnosis, Diffusion processes, {DL}-based active shape model, {DSSD} method, Early vision, edge detection, Equations, Error analysis, example-based super-resolution, extensive dataset, eye fixation prediction, Face, face alignment, Face alignment, face recognition, Face recognition, Feature description, Feature extraction, filter design image analysis, Filtering, filtering and prediction theory, {FMM} method, found in, full 3D data detection, full 3D data segmentation, fuzzy min-max, global feature, global shape regression, graphics, Graphics, Graphics processing unit, Hardware, {HDTV}, hierarchical marginal spaces, High definition video, high frequency details, high-definition television, high-resolution enlargements, histogram, hog, hyperbox, hyperboxes, image based rendering, Image color analysis, Image converters, Image edge detection, Image generation, image resolution, Image resolution, image sampling, image segmentation, image texture, image-based modeling, image-based rendering, image-based representations, Indexes, Internet resource (url), interpolation, intraregion smoothing, Kernel, learning by example, learning classifiers, Learning systems, Local features, machine learning methods, marginal space deep learning, medical image processing, minimax techniques, {MLF}, multilevel fuzzy min-max neural network classifier, multilevel structure, multitask learning framework, Nearest neighbor searches, nearest-neighbor search, neighbouring landmarks, neurofuzzy, neuron, Neurons, nonrigid object boundary, {NTSC} video content conversion, object localization, object parametrization, ocr, of oriented gradients, optical character, orientation analysis, oriented filters, parallel processing, parametrized representations, patient stratification, pattern classification, pattern clustering, person identification, photo browsing, picture processing, pixel-based images, Predictive models, Probabilistischer Algorithmus, Probabilités, Probabilities, probability, psnr, Redundancy, regression analysis, rendering (computer graphics), Rendering (computer graphics), representative image features, restricted affine transformation, {RGB}-D, rihog, Roboter, Robotics, Robotique, rotation-invariant hog, run-time performance, saliency detection., scale-space, scanning high-dimensional parametric spaces, scanning hypotheses, segmentation support, sensing images can be, Shape, shape from shading, single thresholding, Smoothing methods, sparse adaptive data sampling patterns, Streaming media, structural feature learning method, structure from motion, supervised learning, supervised learning method, texture analysis wavelets, texture mapping, therapy planning, three-dimensional (3D) object detection and segmen, Three-dimensional displays, training-based super-resolution algorithm, {TV}, two-step learning problem, ultrasonic imaging, ultrasound, Unsicheres Schließen, Visualization, volumetric medical image data parsing, zoomed images},
}

@article{fei-fei_learning_2004,
	title = {Learning Generative Visual Models from Few Training Examples: An Incremental \{B\}ayesian Approach Tested on 101 Object Categories},
	volume = {106},
	url = {http://dx.doi.org/10.1109/CVPR.2004.109},
	doi = {10.1109/CVPR.2004.109},
	pages = {178},
	number = {1},
	journaltitle = {Conference on Computer Vision and Pattern Recognition Workshop ({CVPR} 2004)},
	author = {Fei-Fei, Li and Fergus, Rod and Perona, Pietro},
	date = {2004},
	note = {{ISBN}: {VO} -
Publisher: Elsevier},
}

@misc{nilsson_introduction_1997,
	title = {Introduction to Machine Learning},
	url = {http://robotics.stanford.edu/{~}nilsson/mlbook.html},
	abstract = {Machine learning and pattern recognition algorithms have in the past years developed to become a working horse in brain imaging and the computational neurosciences, as they are instrumental for mining vast amounts of neural data of ever increasing measurement precision and detecting minuscule signals from an overwhelming noise floor. They provide the means to decode and characterize task relevant brain states and to distinguish them from non-informative brain signals. While undoubtedly this machinery has helped to gain novel biological insights, it also holds the danger of potential unintentional abuse. Ideally machine learning techniques should be usable for any non-expert, however, unfortunately they are typically not. Overfitting and other pitfalls may occur and lead to spurious and nonsensical interpretation. The goal of this review is therefore to provide an accessible and clear introduction to the strengths and also the inherent dangers of machine learning usage in the neurosciences.},
	publisher = {{MIT} press},
	author = {Nilsson, Nils J},
	date = {1997},
	doi = {10.1016/j.neuroimage.2010.11.004},
	pmid = {21172442},
	note = {{ISBN}: 0262012111
{ISSN}: 10959572
Issue: 2
Pages: 387–99
Publication Title: Neural Networks
Volume: 56
\_eprint: 0904.3664v1},
}

@book{szeliski_computer_2010,
	title = {Computer Vision: Algorithms and Applications},
	isbn = {1-84882-934-5},
	url = {http://books.google.com/books?id=216LQgAACAAJ{\&}pgis=1},
	abstract = {As humans, we perceive the three-dimensional structure of the world around us with apparent ease. However, despite all of the recent advances in computer vision research, the dream of having a computer interpret an image at the same level as a two-year old remains elusive. Why is computer vision such a challenging problem, and what is the current state of the art?Computer Vision: Algorithms and Applications explores the variety of techniques commonly used to analyze and interpret images. It also describes challenging real-world applications where vision is being successfully used, both for specialized applications such as medical imaging and fun consumer-level tasks such as image editing and stitching, which students can apply to their own personal photos and videos.More than just a source of "recipes", this text/reference also takes a scientific approach to basic vision problems, formulating physical models of the imaging process before inverting this process to produce the best possible descriptions of a scene. Exercises are presented throughout the book, with a heavy emphasis on testing algorithms.Suitable for either an undergraduate or a graduate-level course in computer vision, this textbook focuses on basic techniques that work under real-world conditions and encourages students to push their creative boundaries.Dr. Richard Szeliski has over twenty years' experience in computer vision research, most notably at Digital Equipment Corporation and Microsoft.},
	pagetotal = {870},
	publisher = {Springer Science \& Business Media},
	author = {Szeliski, Richard},
	date = {2010},
	doi = {10.1007/978-1-84882-935-0},
	pmid = {16259003},
	note = {{ISSN}: 10636919
\_eprint: {arXiv}:1011.1669v3},
}

@book{prince_computer_2013,
	title = {Computer Vision: Models, Learning, and Inference},
	volume = {12},
	isbn = {978-1-107-01179-3},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1474442213700644},
	abstract = {This modern treatment of computer vision focuses on learning and inference in probabilistic models as a unifying theme. It shows how to use training data to learn the relationships between the observed image data and the aspects of the world that we wish to estimate, such as the 3D structure or the object class, and how to exploit these relationships to make new inferences about the world from new image data. With minimal prerequisites, the book starts from the basics of probability and model fitting and works up to real examples that the reader can implement and modify to build useful vision systems. Primarily meant for advanced undergraduate and graduate students, the detailed methodological presentation will also be useful for practitioners of computer vision. - Covers cutting-edge techniques, including graph cuts, machine learning, and multiple view geometry. - A unified approach shows the common basis for solutions of important computer vision problems, such as camera calibration, face recognition, and object tracking. - More than 70 algorithms are described in sufficient detail to implement. - More than 350 full-color illustrations amplify the text. - The treatment is self-contained, including all of the background mathematics. - Additional resources at www.computervisionmodels.com.},
	pagetotal = {335},
	number = {4},
	publisher = {Cambridge University Press},
	author = {Prince, Dr Simon J. D.},
	date = {2013},
	doi = {10.1016/S1474-4422(13)70064-4},
	pmid = {2013185901},
	note = {{ISSN}: 14744422
Publication Title: The Lancet Neurology
\_eprint: {arXiv}:1011.1669v3},
}

@book{nixon_feature_2012,
	title = {Feature Extraction \& Image Processing for Computer Vision},
	isbn = {978-0-12-396549-3},
	url = {https://books.google.com/books?id=lytnomY-r7YC},
	abstract = {This book is an essential guide to the implementation of image processing and computer vision techniques, with tutorial introductions and sample code in Matlab. Algorithms are presented and fully explained to enable complete understanding of the methods and techniques demonstrated. As one reviewer noted, "The main strength of the proposed book is the exemplar code of the algorithms." Fully updated with the latest developments in feature extraction, including expanded tutorials and new techniques, this new edition contains extensive new material on Haar wavelets, Viola-Jones, bilateral filtering, {SURF}, {PCA}-{SIFT}, moving object detection and tracking, development of symmetry operators, {LBP} texture analysis, Adaboost, and a new appendix on color models. Coverage of distance measures, feature detectors, wavelets, level sets and texture tutorials has been extended. Named a 2012 Notable Computer Book for Computing Methodologies by Computing {ReviewsEssential} reading for engineers and students working in this cutting-edge {fieldIdeal} module text and background reference for courses in image processing and computer {visionThe} only currently available text to concentrate on feature extraction with working implementation and worked through derivation},
	pagetotal = {181–182},
	publisher = {Academic Press},
	author = {Nixon, Mark S. and Aguado, Alberto S.},
	date = {2012},
	doi = {http://dx.doi.org/10.1016/B978-0-12-396549-3.00010-0},
}

@article{karen_very_2015,
	title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
	issn = {09505849},
	doi = {10.1016/j.infsof.2008.09.005},
	pages = {1--14},
	author = {Karen, Simonyan and Andrew, Zisserman},
	date = {2015},
	pmid = {16873662},
	note = {{ISBN}: 0950-5849
\_eprint: {arXiv}:1409.1556v6},
}

@article{lucas_iterative_1981,
	title = {An Iterative Image Registration Technique with an Application to Stereo Vision},
	volume = {130},
	pages = {121--129},
	journaltitle = {Imaging},
	author = {Lucas, Bruce D},
	date = {1981},
	note = {Publisher: Vancouver, {BC}, Canada},
}

@article{bay_speeded-up_2008,
	title = {Speeded-Up Robust Features ({SURF})},
	volume = {110},
	issn = {10773142},
	doi = {10.1016/j.cviu.2007.09.014},
	abstract = {This article presents a novel scale- and rotation-invariant detector and descriptor, coined {SURF} (Speeded-Up Robust Features). {SURF} approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (specifically, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper encompasses a detailed description of the detector and descriptor and then explores the effects of the most important parameters. We conclude the article with {SURF}'s application to two challenging, yet converse goals: camera calibration as a special case of image registration, and object recognition. Our experiments underline {SURF}'s usefulness in a broad range of topics in computer vision. ?? 2007 Elsevier Inc. All rights reserved.},
	pages = {346--359},
	number = {3},
	journaltitle = {Computer Vision and Image Understanding},
	author = {Bay, Herbert and Ess, Andreas and Tuytelaars, Tinne and Van Gool, Luc},
	date = {2008},
	pmid = {16081019},
	note = {{ISBN}: 9783540338321},
	keywords = {Object recognition, Interest points, Camera calibration, Feature description, Local features},
}

@inproceedings{gortler_lumigraph_1996,
	title = {The lumigraph},
	isbn = {0-89791-746-4},
	url = {http://portal.acm.org/citation.cfm?doid=237170.237200},
	doi = {10.1145/237170.237200},
	pages = {43--54},
	booktitle = {the 23rd annual conference on Computer graphics and interactive techniques},
	publisher = {{ACM}},
	author = {Gortler, Steven J. and Grzeszczuk, Radek and Szeliski, Richard and Cohen, Michael F.},
	date = {1996},
	note = {{ISSN}: 00978930},
}

@inproceedings{snavely_photo_2006,
	title = {Photo tourism: Exploring Photo Collections in 3D},
	volume = {25},
	isbn = {1-59593-364-6},
	url = {http://doi.acm.org/10.1145/1141911.1141964{\%}5Cnhttp://portal.acm.org/citation.cfm?doid=1141911.1141964},
	doi = {10.1145/1141911.1141964},
	abstract = {We present a system for interactively browsing and exploring large unstructured collections of photographs of a scene using a novel 3D interface. Our system consists of an image-based modeling front end that automatically computes the viewpoint of each photograph as well as a sparse 3D model of the scene and image to model correspondences. Our photo explorer uses image-based rendering techniques to smoothly transition between photographs, while also enabling full 3D navigation and exploration of the set of images and world geometry, along with auxiliary information such as overhead maps. Our system also makes it easy to construct photo tours of scenic or historic locations, and to annotate image details, which are automatically transferred to other relevant images. We demonstrate our system on several large personal photo collections as well as images gathered from Internet photo sharing sites.},
	pages = {835--846},
	booktitle = {{ACM} Transactions on Graphics},
	publisher = {{ACM}},
	author = {Snavely, Noah and Seitz, Steven M and Szeliski, Richard},
	date = {2006},
	pmid = {18787244},
	note = {{ISSN}: 07300301
Issue: 3},
	keywords = {image-based modeling, image-based rendering, photo browsing, structure from motion},
}

@article{kass_snakes_1988,
	title = {Snakes: active contour model},
	volume = {1},
	pages = {321--331},
	number = {4},
	journaltitle = {International Journal on Computer Vision},
	author = {Kass, M and Witkin, A and Terzopoulos, D},
	date = {1988},
}

@article{r_m_haralick_i_dinstein_textural_1973,
	title = {Textural features for image classification},
	volume = {3},
	pages = {610--621},
	number = {6},
	author = {R M Haralick, I Dinstein, K Shanmugam},
	date = {1973},
	note = {Publisher: Ieee},
}

@article{zhao_face_2003,
	title = {Face Recognition: A Literature Survey},
	volume = {35},
	issn = {0360-0300},
	url = {http://www.face-rec.org/interesting-papers/General/zhao00face.pdf{\%}5Cnhttp://portal.acm.org/citation.cfm?id=954342{\%}5Cnhttp://doi.acm.org/10.1145/954339.954342},
	doi = {10.1145/954339.954342},
	abstract = {As one of the most successful applications of image analysis and understanding, face recognition has recently received significant attention, especially during the past several years. At least two reasons account for this trend: the first is the wide range of commercial and law enforcement applications, and the second is the availability of feasible technologies after 30 years of research. Even though current machine recognition systems have reached a certain level of maturity, their success is limited by the conditions imposed by many real applications. For example, recognition of face images acquired in an outdoor environment with changes in illumination and/or pose remains a largely unsolved problem. In other words, current systems are still far away from the capability of the human perception system.This paper provides an up-to-date critical survey of still- and video-based face recognition research. There are two underlying motivations for us to write this survey paper: the first is to provide an up-to-date review of the existing literature, and the second is to offer some insights into the studies of machine recognition of faces. To provide a comprehensive survey, we not only categorize existing recognition techniques but also present detailed descriptions of representative methods within each category. In addition, relevant topics such as psychophysical studies, system evaluation, and issues of illumination and pose variation are covered.},
	pages = {399--458},
	number = {4},
	journaltitle = {{ACM} Comput. Surv.},
	author = {Zhao, W. and Chellappa, R. and Phillips, P. J. and Rosenfeld, a. and Zhao, W. and Chellappa, R. and Chellappa, R. and Phillips, P. J. and Phillips, P. J. and Rosenfeld, a. and Rosenfeld, a.},
	date = {2003},
	pmid = {19556198},
	note = {{ISBN}: 0360-0300
Publisher: {ACM}},
	keywords = {Face recognition, person identification},
}

@article{freeman_design_1991,
	title = {The Design and Use of Steerable Filters Copyright 1 Introduction},
	volume = {13},
	pages = {891--906},
	number = {9},
	journaltitle = {Technology},
	author = {Freeman, William T and Adelson, Edward H},
	date = {1991},
}

@misc{thrun_probabilistic_2005,
	title = {Probabilistic robotics},
	abstract = {Probablistic robotics is a growing area in the subject, concerned with perception and control in the face of uncertainty and giving robots a level of robustness in real-world situations. This book introduces techniques and algorithms in the field.; Recursive state estimation – Gaussian filters – Robot motion – Robot perception – Mobile robot localization : Markov and Gaussian – Mobile robot localization : grid and Monte Carlo – Occupancy grid mapping – Simultaneous localization and mapping – The {graphSLAM} algorithm – The sparse extended information filter – The {fastSLAM} algorithm – Markov decision processes – Partially observable Markov decision processes – Approximate {POMDP} techniques – Exploration.},
	publisher = {{MIT} Press},
	author = {Thrun, Sebastian and Burgard, Wolfram and Fox, Dieter},
	date = {2005},
	doi = {10.1145/504729.504754},
	pmid = {25246403},
	note = {{ISBN}: 0262201623 (alk. paper); 9780262201629 (alk. paper)
{ISSN}: 00010782
Pages: 647
Publication Title: Intelligent robotics and autonomous agents; Variation: Intelligent robotics and autonomous agents.
\_eprint: {arXiv}:1011.1669v3},
	keywords = {Bayes-Verfahren, {CAR} (Roboter), Internet resource (url), Probabilistischer Algorithmus, Probabilités, Probabilities, Roboter, Robotics, Robotique, Unsicheres Schließen},
}

@article{breitenreiter_deep_2015,
	title = {Deep Learning},
	volume = {521},
	issn = {1548-7091},
	doi = {10.1038/nmeth.3707},
	pages = {2015},
	number = {7553},
	author = {Breitenreiter, Anselm and Poppinga, Heiko and Berlin, T U and Technik, Fachgebiet Nachrichten},
	date = {2015},
	pmid = {10463930},
	note = {{ISBN}: 9780521835688
Publisher: Nature Research
\_eprint: {arXiv}:1312.6184v5},
}

@article{pirahansiah_peak_2013,
	title = {Peak signal-to-noise ratio based on threshold method for image segmentation},
	volume = {57},
	rights = {All rights reserved},
	issn = {18173195},
	pages = {158--168},
	number = {2},
	journaltitle = {Journal of Theoretical and Applied Information Technology},
	author = {Pirahansiah, Farshid and Abdullah, Siti Norul Huda Sheikh and Sahran, Shahnorbanun},
	date = {2013},
	note = {Publisher: www.jatit.org/volumes/Vol57No2/4Vol57No2.pdf},
	keywords = {Image processing, Image segmentation, Optical character recognition, Single thresholding, {PSNR}},
	file = {Attachment:C\:\\Users\\farshidp\\Zotero\\storage\\2QK5WQL8\\PEAK SIGNAL-TO-NOISE RATIO BASED ON THRESHOLD METHOD FOR IMAGE SEGMENTATION_2013.pdf:application/pdf},
}

@article{naeimizaghiani_character_2011,
	title = {Character recognition based on global feature extraction},
	volume = {52},
	rights = {All rights reserved},
	issn = {21556822},
	url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5665123},
	abstract = {This paper presents a enhanced feature extraction method which is a combination and selected of two feature extraction techniques of Gray Level Co occurrence Matrix ({GLCM}) and Edge Direction Matrixes ({EDMS}) for character recognition purpose. It is apparent that one of the most important steps in a character recognition system is selecting a better feature extraction technique, while the variety of method makes difficulty for finding the best techniques for character recognition. The dataset of images that has been applied to the different feature extraction techniques includes the binary character with different sizes. Experimental results show the better performance of proposed method in compared with {GLCM} and {EDMS} method after performing the feature selection with neural network, bayes network and decision tree classifiers},
	pages = {1--4},
	number = {2},
	journaltitle = {Proceedings of the 2011 International Conference on Electrical Engineering and Informatics},
	author = {Naeimizaghiani, Maryam and Abdullah, Siti Norul Huda Sheikh and Bataineh, Bilal and {PirahanSiah}, Farshid},
	date = {2011},
	note = {{ISBN}: 9781457707513},
	keywords = {feature extraction, image processing, character recognition, ocr},
}

@inproceedings{alex_imagenet_2012,
	title = {Imagenet classification with deep convolutional neural networks},
	url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
	pages = {1097--1105},
	booktitle = {Neural Information Processing Systems ({NIPS})},
	author = {Alex, Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E.},
	date = {2012},
}

@article{schmidhuber_multi-column_2012,
	title = {Multi-column deep neural networks for image classification},
	url = {http://dl.acm.org/citation.cfm?id=2354409.2354694},
	doi = {10.1109/CVPR.2012.6248110},
	abstract = {Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible, wide and deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive {MNIST} handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks},
	pages = {3642--3649},
	journaltitle = {{IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR}),},
	author = {Schmidhuber, Jurgen},
	date = {2012},
	note = {{ISBN}: 978-1-4673-1226-4
\_eprint: {arXiv}:1202.2745v1},
	keywords = {Training, Benchmark testing, Computer architecture, Error analysis, Graphics processing unit, Neurons},
}

@article{haralick_statistical_1979,
	title = {Statistical and structural approaches to texture},
	volume = {67},
	issn = {15582256},
	doi = {10.1109/PROC.1979.11328},
	abstract = {In this survey we review the image processing literature on the various approaches and models investigators have used for texture. These include statistical approaches of autocorrelation function, optical transforms, digital transforms, textural edgeness, structural element, gray tone cooccurrence, run lengths, and autoregressive models. We discuss and generalize some structural approaches to texture based on more complex primitives than gray tone. We conclude with some structural-statistical generalizations which apply the statistical techniques to the structural primitives.},
	pages = {786--804},
	number = {5},
	journaltitle = {Proceedings of the {IEEE}},
	author = {Haralick, Robert M.},
	date = {1979},
	note = {{ISBN}: 0018-9219
Publisher: {IEEE}},
}

@article{intelligence_modern_2003,
	title = {A modern approach},
	volume = {25},
	url = {http://scholar.google.com/scholar?hl=en{\&}q=artificial+intelligence+A+Modern+Approach{\&}btnG={\&}as{\_}sdt=1,5{\&}as{\_}sdtp={\#}2},
	pages = {1--5},
	journaltitle = {Russell and Norvig},
	author = {Intelligence, A},
	date = {2003},
	note = {{ISBN}: 0131038052},
}

@book{trucco_introductory_1998,
	title = {Introductory Techniques for 3D Computer Vision},
	volume = {201},
	isbn = {0-13-261108-2},
	publisher = {Prentice Hall Englewood Cliffs},
	author = {Trucco, Emanuele and Alessandro, Verri},
	date = {1998},
	note = {Publication Title: Prentice-Hall.},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {00189219},
	doi = {10.1109/5.726791},
	abstract = {Multilayer neural networks trained with the back-propagation\${\textbackslash}backslash\$nalgorithm constitute the best example of a successful gradient based\${\textbackslash}backslash\$nlearning technique. Given an appropriate network architecture,\${\textbackslash}backslash\$ngradient-based learning algorithms can be used to synthesize a complex\${\textbackslash}backslash\$ndecision surface that can classify high-dimensional patterns, such as\${\textbackslash}backslash\$nhandwritten characters, with minimal preprocessing. This paper reviews\${\textbackslash}backslash\$nvarious methods applied to handwritten character recognition and\${\textbackslash}backslash\$ncompares them on a standard handwritten digit recognition task.\${\textbackslash}backslash\${nConvolutional} neural networks, which are specifically designed to deal\${\textbackslash}backslash\$nwith the variability of 2D shapes, are shown to outperform all other\${\textbackslash}backslash\$ntechniques. Real-life document recognition systems are composed of\${\textbackslash}backslash\$nmultiple modules including field extraction, segmentation recognition,\${\textbackslash}backslash\$nand language modeling. A new learning paradigm, called graph transformer\${\textbackslash}backslash\$nnetworks ({GTN}), allows such multimodule systems to be trained globally\${\textbackslash}backslash\$nusing gradient-based methods so as to minimize an overall performance\${\textbackslash}backslash\$nmeasure. Two systems for online handwriting recognition are described.\${\textbackslash}backslash\${nExperiments} demonstrate the advantage of global training, and the\${\textbackslash}backslash\$nflexibility of graph transformer networks. A graph transformer network\${\textbackslash}backslash\$nfor reading a bank cheque is also described. It uses convolutional\${\textbackslash}backslash\$nneural network character recognizers combined with global training\${\textbackslash}backslash\$ntechniques to provide record accuracy on business and personal cheques.\${\textbackslash}backslash\${nIt} is deployed commercially and reads several million cheques per day\${\textbackslash}backslash\$n},
	pages = {2278--2323},
	number = {11},
	journaltitle = {Proceedings of the {IEEE}},
	author = {{LeCun}, Yann and Bottou, L??on and Bengio, Yoshua and Haffner, Patrick},
	date = {1998},
	pmid = {15823584},
	note = {{ISBN}: 0018-9219
Publisher: {IEEE}
\_eprint: 1102.0183},
	keywords = {Machine learning, Neural networks, Convolutional neural networks, Document recognition, Finite state transducers, Gradient-based learning, Graph transformer networks, Optical character recognition ({OCR})},
}

@article{hinton_fast_2006,
	title = {A fast learning algorithm for deep belief nets},
	volume = {18},
	issn = {0899-7667},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/neco.2006.18.7.1527},
	doi = {10.1162/neco.2006.18.7.1527},
	abstract = {We show how to use complementary priors to eliminate the explaining- away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associa- tive memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive ver- sion of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribu- tion of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning al- gorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
	pages = {1527--1554},
	number = {7},
	journaltitle = {Neural computation},
	author = {Hinton, {GE} and Osindero, Simon and Teh, {YW}},
	date = {2006-05},
	pmid = {16764513},
	note = {{ISBN}: 0899-7667
Publisher: {MIT} Press
\_eprint: 1111.6189v1},
}

@article{liu_learning_2016,
	title = {Learning to Predict Eye Fixations via Multiresolution Convolutional Neural Networks},
	volume = {{PP}},
	issn = {2162-237X},
	doi = {10.1109/TNNLS.2016.2628878},
	abstract = {Eye movements in the case of freely viewing natural scenes are believed to be guided by local contrast, global contrast, and top-down visual factors. Although a lot of previous works have explored these three saliency cues for several years, there still exists much room for improvement on how to model them and integrate them effectively. This paper proposes a novel computation model to predict eye fixations, which adopts a multiresolution convolutional neural network (Mr-{CNN}) to infer these three types of saliency cues from raw image data simultaneously. The proposed Mr-{CNN} is trained directly from fixation and nonfixation pixels with multiresolution input image regions with different contexts. It utilizes image pixels as inputs and eye fixation points as labels. Then, both the local and global contrasts are learned by fusing information in multiple contexts. Meanwhile, various top-down factors are learned in higher layers. Finally, optimal combination of top-down factors and bottom-up contrasts can be learned to predict eye fixations. The proposed approach significantly outperforms the state-of-the-art methods on several publically available benchmark databases, demonstrating the superiority of Mr-{CNN}. We also apply our method to the {RGB}-D image saliency detection problem. Through learning saliency cues induced by depth and {RGB} information on pixel level jointly and their interactions, our model achieves better performance on predicting eye fixations in {RGB}-D images.},
	pages = {1--13},
	number = {99},
	journaltitle = {{IEEE} Transactions on Neural Networks and Learning Systems},
	author = {Liu, N. and Han, J. and Liu, T. and Li, X.},
	date = {2016},
	keywords = {Context, Contrast, convolutional neural network ({CNN}), eye fixation prediction, Feature extraction, Image color analysis, Image resolution, Predictive models, {RGB}-D, saliency detection., Streaming media, Visualization},
}

@inproceedings{he_delving_2015,
	title = {Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
	pages = {1026--1034},
	booktitle = {Proceedings of the {IEEE} international conference on computer vision},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	date = {2015},
}

@article{xu_empirical_2015,
	title = {Empirical evaluation of rectified activations in convolutional network},
	journaltitle = {{arXiv} preprint {arXiv}:1505.00853},
	author = {Xu, Bing and Wang, Naiyan and Chen, Tianqi and Li, Mu},
	date = {2015},
}

@article{srivastava_dropout_2014,
	title = {Dropout: a simple way to prevent neural networks from overfitting.},
	volume = {15},
	pages = {1929--1958},
	number = {1},
	journaltitle = {Journal of Machine Learning Research},
	author = {Srivastava, Nitish and Hinton, Geoffrey E and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	date = {2014},
}

@inproceedings{wei_deep_2015,
	title = {Deep spatial pyramid ensemble for cultural event recognition},
	pages = {38--44},
	booktitle = {Proceedings of the {IEEE} International Conference on Computer Vision Workshops},
	author = {Wei, Xiu-Shen and Gao, Bin-Bin and Wu, Jianxin},
	date = {2015},
}

@book{zhou_ensemble_2012,
	title = {Ensemble methods: foundations and algorithms},
	publisher = {{CRC} press},
	author = {Zhou, Zhi-Hua},
	date = {2012},
}

@misc{masko_impact_2015,
	title = {The impact of imbalanced training data for convolutional neural networks},
	author = {Masko, David and Hensman, Paulina},
	date = {2015},
}

@incollection{goodfellow_generative_2014,
	title = {Generative Adversarial Nets},
	url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
	pages = {2672--2680},
	booktitle = {Advances in Neural Information Processing Systems 27},
	publisher = {Curran Associates, Inc.},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	date = {2014},
}

@incollection{denton_deep_2015,
	title = {Deep Generative Image Models using a ￼Laplacian Pyramid of Adversarial Networks},
	url = {http://papers.nips.cc/paper/5773},
	pages = {1486--1494},
	booktitle = {Advances in Neural Information Processing Systems 28},
	publisher = {Curran Associates, Inc.},
	author = {Denton, Emily L and Chintala, Soumith and szlam, arthur and Fergus, Rob},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	date = {2015},
}

@article{hajigholam_multitask_2016,
	title = {Multitask joint spatial pyramid matching using sparse representation with dynamic coefficients for object recognition},
	volume = {25},
	pages = {023019--023019},
	number = {2},
	journaltitle = {Journal of Electronic Imaging},
	author = {Hajigholam, Mohammad-Hossein and Raie, Abolghasem-Asadollah and Faez, Karim},
	date = {2016},
	note = {Publisher: International Society for Optics and Photonics},
}

@article{banerji_new_2013,
	title = {New image descriptors based on color, texture, shape, and wavelets for object and scene image classification},
	volume = {117},
	pages = {173--185},
	journaltitle = {Neurocomputing},
	author = {Banerji, Sugata and Sinha, Atreyee and Liu, Chengjun},
	date = {2013},
	note = {Publisher: Elsevier},
}

@inproceedings{bosch_image_2007,
	title = {Image classification using random forests and ferns},
	pages = {1--8},
	booktitle = {Computer Vision, 2007. {ICCV} 2007. {IEEE} 11th International Conference on},
	publisher = {{IEEE}},
	author = {Bosch, Anna and Zisserman, Andrew and Munoz, Xavier},
	date = {2007},
}

@article{torralba_80_2008,
	title = {80 million tiny images: A large data set for nonparametric object and scene recognition},
	volume = {30},
	pages = {1958--1970},
	number = {11},
	journaltitle = {{IEEE} transactions on pattern analysis and machine intelligence},
	author = {Torralba, Antonio and Fergus, Rob and Freeman, William T},
	date = {2008},
	note = {Publisher: {IEEE}},
}

@inproceedings{sinha_novel_2012,
	title = {Novel color Gabor-{LBP}-{PHOG} ({GLP}) descriptors for object and scene image classification},
	pages = {58},
	booktitle = {Proceedings of the Eighth Indian Conference on Computer Vision, Graphics and Image Processing},
	publisher = {{ACM}},
	author = {Sinha, Atreyee and Banerji, Sugata and Liu, Chengjun},
	date = {2012},
}

@article{van_gemert_visual_2010,
	title = {Visual word ambiguity},
	volume = {32},
	pages = {1271--1283},
	number = {7},
	journaltitle = {{IEEE} transactions on pattern analysis and machine intelligence},
	author = {Van Gemert, Jan C and Veenman, Cor J and Smeulders, Arnold {WM} and Geusebroek, Jan-Mark},
	date = {2010},
	note = {Publisher: {IEEE}},
}

@article{chen_beyond_2017,
	title = {Beyond triplet loss: a deep quadruplet network for person re-identification},
	journaltitle = {{arXiv} preprint {arXiv}:1704.01719},
	author = {Chen, Weihua and Chen, Xiaotang and Zhang, Jianguo and Huang, Kaiqi},
	date = {2017},
}

@article{drummond_c45_2003,
	title = {C4.5, Class Imbalance, and Cost Sensitivity: Why Under-Sampling beats {OverSampling}},
	journaltitle = {Proceedings of the {ICML}'03 Workshop on Learning from Imbalanced Datasets},
	author = {Drummond, Chris and C. Holte, Robert},
	date = {2003-01},
}

@article{simonyan_very_2014,
	title = {Very deep convolutional networks for large-scale image recognition},
	journaltitle = {{arXiv} preprint {arXiv}:1409.1556},
	author = {Simonyan, Karen and Zisserman, Andrew},
	date = {2014},
}

@inproceedings{pirahansiah_camera_2015,
	title = {Camera calibration for multi-modal robot vision based on image quality assessment},
	rights = {All rights reserved},
	doi = {10.1109/ASCC.2015.7360336},
	abstract = {Multi-dimension robot vision in autonomous humanoid robot is still an open issue as it performs less effective when dealing with different environments. Robot vision becomes more challenging as image quality degrades. Unlike human vision, current robot vision is yet to calibrate automatically when image quality changes abruptly. This may result in poor accuracy due to false negative input data points, and the user needs recapturing new calibration images to compensate. Therefore, this study emphasizes on proposing an automatic calibration for multimodal robot vision based on quality measures. We organize our research methodology into three steps. First, we capture a series of image patterns by using our calibration pattern equipment. Second, we employ Image Quality Assessment Function ({IQAF}) that includes {PSNR} and {SSIM} to measure points of image abruption simultaneously. In the experiment, we observed differences between real distance and computed distance and compared them to those of the selfcollected original database and the blur database.},
	pages = {1--6},
	booktitle = {2015 10th Asian Control Conference ({ASCC})},
	author = {Pirahansiah, F. and Abdullah, S. N. H. S. and Sahran, S.},
	date = {2015-05},
	keywords = {humanoid robot, Mathematical model, Three-dimensional displays, {PSNR}, 3D vision, automatic camera calibration, autonomous humanoid robot, binocular vision, blur database, calibration, Calibration, calibration images, calibration pattern equipment, Camera re-sectioning, cameras, Cameras, create datasets, geometric Camera Calibration, human vision, humanoid robots, image abruption, image patterns, Image quality, image quality assessment function, {IQAF}, multimodal robot vision, robot vision, Robot vision systems, self-collected original database, {SSIM}, stereo vision, Stereo vision, stereopsis calibration},
}

@inproceedings{pirahansiah_pattern_2017,
	title = {Pattern image significance for camera calibration},
	rights = {All rights reserved},
	doi = {10.1109/SCORED.2017.8305440},
	abstract = {The image information from cameras can yield geometric information pertaining to three-dimensional objects by having camera parameters. Camera Calibration is a method for calculating the parameters of a pinhole camera model. Several methods used for camera calibration which are self-calibration, active vision, and known objects. Usually known object pattern uses calibration pattern such as chessboard. Furthermore, another important element is the number of image selection that also adheres better impact to overall of accuracy rate. Therefore, we categorize and explain each method in camera calibration in this paper. Finally, we show the significance of number of image and slope in camera calibration in several experimental result to justify our claim.},
	pages = {456--460},
	booktitle = {2017 {IEEE} 15th Student Conference on Research and Development ({SCOReD})},
	author = {{PirahanSiah}, F. and Abdullah, S. N. H. S. and Sahran, S.},
	date = {2017-12},
	keywords = {Three-dimensional displays, calibration, Calibration, cameras, Cameras, Robot vision systems, 3D, 3D objects, active vision, adaptive parameter setting, calibration pattern, camera calibration, Camera Calibration, camera parameters, Conferences, Distortion, geometry, image selection, pattern image significance, pinhole camera model, reconstruction, Research and development, stereo image processing},
}

@article{asgari_using_2017,
	title = {Using an ant colony optimization algorithm for image edge detection as a threshold segmentation for {OCR} system},
	volume = {95},
	rights = {All rights reserved},
	pages = {5654--5664},
	number = {21},
	journaltitle = {Journal of Theoretical and Applied Information Technology},
	author = {Asgari, Maryam and Pirahansiah, Farshid and Shahverdy, Mohammad and Fartash, Mehdi},
	date = {2017},
	file = {Attachment:C\:\\Users\\farshidp\\Zotero\\storage\\36V7GGDR\\USING AN ANT COLONY OPTIMIZATION ALGORITHM_2017_1Vol95No21.pdf:application/pdf},
}

@article{naeimizaghiani_character_2013,
	title = {Character and Object Recognition Based on Global Feature Extraction},
	volume = {52},
	rights = {All rights reserved},
	number = {2},
	journaltitle = {Journal of Theoretical \& Applied Information Technology},
	author = {Naeimizaghiani, Maryam and Abdullah, Siti Norul Huda Sheikh and Pirahansiah, Farshid and Bataineh, Bilal},
	date = {2013},
	file = {Attachment:C\:\\Users\\farshidp\\Zotero\\storage\\KRSF32YQ\\CHARACTER AND OBJECT RECOGNITION BASED ON GLOBAL FEATURE EXTRACTION_2013.pdf:application/pdf},
}

@inproceedings{abidin_license_2011,
	title = {License plate recognition with multi-threshold based on entropy},
	rights = {All rights reserved},
	doi = {10.1109/ICEEI.2011.6021627},
	abstract = {Among all the existing segmentation techniques, thresholding technique is one of the most popular one due to its simplicity, robustness and accuracy. Multi-thresholding is an important operation in many analyses which is used in many applications. Selecting correct thresholds to get better result is a critical issue. In this research, a multilevel thresholding method is proposed based on combination of maximum entropy. The maximum entropy thresholding algorithm selects several threshold values by maximizing the cross entropy between the original image and the segmented image. This method can effectively integrate partial range of the image histogram. The proposed algorithm is compared with single thresholding method based on maximum entropy and multilevel thresholding method The proposed multi thresholding method is tested on license plate application. From the experiment, multi-threshold method further improved to increase the segmentation accuracy in the future.},
	pages = {1--6},
	booktitle = {Proceedings of the 2011 International Conference on Electrical Engineering and Informatics},
	author = {Abidin, N. H. Zainal and Abdullah, S. N. H. S. and Sahran, S. and {PirahanSiah}, F.},
	date = {2011-07},
	keywords = {Image segmentation, Accuracy, Feature extraction, image segmentation, {PSNR}, Character recognition, Entropy, image histogram, license plate recognition, Licenses, maximum entropy methods, maximum entropy thresholding algorithm, multilevel thresholding method, multithreshold technique, {OCR}, optical character recognition, original image, peak signal to noise ratio, segmentation, single thresholding method, thresholding},
}

@inproceedings{pirahansiah_adaptive_2010,
	title = {Adaptive image segmentation based on peak signal-to-noise ratio for a license plate recognition system},
	rights = {All rights reserved},
	doi = {10.1109/ICCAIE.2010.5735125},
	abstract = {The objective of this paper is to propose an adaptive threshold method based on peak signal to noise ratio ({PSNR}). Nowadays, {PSNR} has been widely used as stopping criteria in multi level threshold method for segmenting images. Alternatively, we apply the {PSNR} as criteria to find the most suitable threshold value. We evaluate this proposed method on license plate recognition application. At the same time, we compare this proposed algorithm with multi-level and multi-threshold methods as the benchmark. Via the proposed technique, it could relatively change according to environment such as when there is a high or low contrast situation.},
	pages = {468--472},
	booktitle = {2010 International Conference on Computer Applications and Industrial Electronics},
	author = {{PirahanSiah}, F. and Abdullah, S. N. H. S. and Sahran, S.},
	date = {2010-12},
	keywords = {Image segmentation, Accuracy, image segmentation, {PSNR}, license plate recognition, Licenses, adaptive image segmentation, adaptive multilevel threshold method, Adaptive threshold, image recognition, Image recognition, license plate recognition system, Optical character recognition software, Pattern recognition, peak signal-to-noise ratio, thresholding image segmentation, traffic engineering computing},
}

@book{farshid_pirahansiah_chapter_2017,
	title = {{CHAPTER} 4: Augmented Optical Flow Methods for Video Stabilization Computational Intelligence: from theory to application},
	rights = {All rights reserved},
	isbn = {978-967-412-458-8},
	publisher = {Penerbit Universiti Kebangsaan Malaysia},
	author = {Farshid {PirahanSiah}, S.N.H.S. Abdullah, Shahnorbanun Sahran},
	editor = {Aris, Siti Norul Huda Sheikh {AbdullahSeyed} Mostafa Mousavi Kahaki Akmal},
	date = {2017},
}

@inproceedings{asgari_tafreshgrid_2011,
	title = {{TafreshGrid}: Grid computing in Tafresh university},
	rights = {All rights reserved},
	pages = {83--85},
	booktitle = {2011 {IEEE} 3rd International Conference on Communication Software and Networks},
	publisher = {{IEEE}},
	author = {Asgari, Maryam and Shahverdy, Mohammad and Pirahansiah, Farshid and Mahdavi, Zahra},
	date = {2011},
}

@inproceedings{pirahansiah_2d_2012,
	title = {2D versus 3D map for environment movement object},
	rights = {All rights reserved},
	booktitle = {2nd National Doctoral Seminar on Artificial Intelligence Technology, Residence Hotel, {UNITEN}, Malaysia},
	author = {{PirahanSiah}, Farshid and Abdullah, {SNHS} and Sahran, S},
	date = {2012},
}

@article{chen_simple_2020,
	title = {A Simple Framework for Contrastive Learning of Visual Representations},
	abstract = {This paper presents {SimCLR}: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on {ImageNet}. A linear classifier trained on self-supervised representations learned by {SimCLR} achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised {ResNet}-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming {AlexNet} with 100X fewer labels.},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	date = {2020-02-13},
	eprinttype = {arxiv},
	eprint = {2002.05709v2},
	keywords = {cs.{LG}, cs.{CV}, stat.{ML}},
}

@article{liu_learning_2017,
	title = {Learning Deep Sharable and Structural Detectors for Face Alignment},
	volume = {26},
	issn = {1057-7149},
	url = {http://ieeexplore.ieee.org/document/7829264/},
	doi = {10.1109/TIP.2017.2657118},
	abstract = {Face alignment aims at localizing multiple facial landmarks for a given facial image, which usually suffers from large variances of diverse facial expressions, aspect ratios and partial occlusions, especially when face images were captured in wild conditions. Conventional face alignment methods extract local features and then directly concatenate these features for global shape regression. Unlike these methods which cannot explicitly model the correlation of neighbouring landmarks and motivated by the fact that individual landmarks are usually correlated, we propose a deep sharable and structural detectors ({DSSD}) method for face alignment. To achieve this, we firstly develop a structural feature learning method to explicitly exploit the correlation of neighbouring landmarks, which learns to cover semantic information to disambiguate the neighbouring landmarks. Moreover, our model selectively learns a subset of sharable latent tasks across neighbouring landmarks under the paradigm of the multi-task learning framework, so that the redundancy information of the overlapped patches can be efficiently removed. To better improve the performance, we extend our {DSSD} to a recurrent {DSSD} (R-{DSSD}) architecture by integrating with the complementary information from multi-scale perspectives. Experimental results on the widely used benchmark datasets show that our methods achieve very competitive performance compared to the state-of-the-arts.},
	pages = {1--1},
	number = {4},
	journaltitle = {{IEEE} Transactions on Image Processing},
	author = {Liu, Hao and Lu, Jiwen and Feng, Jianjiang and Zhou, Jie},
	date = {2017},
	keywords = {feature extraction, face recognition, learning (arti},
}

@patent{mnih_methods_2017,
	title = {Methods and apparatus for reinforcement learning},
	type = {patent},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray},
	date = {2017-06-13},
	note = {Publisher: Google Patents},
}

@patent{ghesu_multi-scale_2018,
	title = {Multi-scale deep reinforcement machine learning for N-dimensional segmentation in medical imaging},
	type = {patent},
	author = {Ghesu, Florin Cristian and Georgescu, Bogdan and Comaniciu, Dorin},
	date = {2018-07-24},
	note = {Publisher: Google Patents},
}

@misc{nguyen_review_2020,
	title = {Review, Analyze, and Design a Comprehensive Deep Reinforcement Learning Framework},
	author = {Nguyen, Ngoc Duy and Nguyen, Thanh Thi and Nguyen, Hai and Nahavandi, Saeid},
	date = {2020},
	note = {\_eprint: 2002.11883},
}

@article{francois-lavet_introduction_2018,
	title = {An Introduction to Deep Reinforcement Learning},
	volume = {11},
	issn = {1935-8245},
	url = {http://dx.doi.org/10.1561/2200000071},
	doi = {10.1561/2200000071},
	pages = {219--354},
	number = {3},
	journaltitle = {Foundations and Trends® in Machine Learning},
	author = {François-Lavet, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G. and Pineau, Joelle},
	date = {2018},
	note = {Publisher: Now Publishers},
}

@misc{nagaraja_processor_2017,
	title = {Processor for implementing reinforcement learning operations},
	publisher = {Google Patents},
	author = {Nagaraja, Nagendra},
	date = {2017-09-05},
}

@article{kaelbling_reinforcement_1996,
	title = {Reinforcement learning: A survey},
	volume = {4},
	pages = {237--285},
	journaltitle = {Journal of artificial intelligence research},
	author = {Kaelbling, Leslie Pack and Littman, Michael L and Moore, Andrew W},
	date = {1996},
}

@inproceedings{lange_deep_2010,
	title = {Deep auto-encoder neural networks in reinforcement learning},
	pages = {1--8},
	booktitle = {The 2010 International Joint Conference on Neural Networks ({IJCNN})},
	publisher = {{IEEE}},
	author = {Lange, Sascha and Riedmiller, Martin},
	date = {2010},
}

@article{zhou_video_2018,
	title = {Video summarisation by classification with deep reinforcement learning},
	journaltitle = {{arXiv} preprint {arXiv}:1807.03089},
	author = {Zhou, Kaiyang and Xiang, Tao and Cavallaro, Andrea},
	date = {2018},
}

@article{gruslys_reactor_2017,
	title = {The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning},
	journaltitle = {{arXiv} preprint {arXiv}:1704.04651},
	author = {Gruslys, Audrunas and Dabney, Will and Azar, Mohammad Gheshlaghi and Piot, Bilal and Bellemare, Marc and Munos, Remi},
	date = {2017},
}

@article{machado_revisiting_2018,
	title = {Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents},
	volume = {61},
	pages = {523--562},
	journaltitle = {Journal of Artificial Intelligence Research},
	author = {Machado, Marlos C and Bellemare, Marc G and Talvitie, Erik and Veness, Joel and Hausknecht, Matthew and Bowling, Michael},
	date = {2018},
}

@inproceedings{hessel_rainbow_2018,
	title = {Rainbow: Combining improvements in deep reinforcement learning},
	booktitle = {Thirty-Second {AAAI} Conference on Artificial Intelligence},
	author = {Hessel, Matteo and Modayil, Joseph and Van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
	date = {2018},
}

@article{salimans_evolution_2017,
	title = {Evolution strategies as a scalable alternative to reinforcement learning},
	journaltitle = {{arXiv} preprint {arXiv}:1703.03864},
	author = {Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya},
	date = {2017},
}

@inproceedings{irpan_off-policy_2019,
	title = {Off-policy evaluation via off-policy classification},
	pages = {5438--5449},
	booktitle = {Advances in Neural Information Processing Systems},
	author = {Irpan, Alexander and Rao, Kanishka and Bousmalis, Konstantinos and Harris, Chris and Ibarz, Julian and Levine, Sergey},
	date = {2019},
}

@article{bellman_markov_1957,
	title = {A markov decision process. journal of Mathematical Mechanics},
	author = {Bellman, {RE}},
	date = {1957},
}

@article{tian_elf_2019,
	title = {Elf opengo: An analysis and open reimplementation of alphazero},
	journaltitle = {{arXiv} preprint {arXiv}:1902.04522},
	author = {Tian, Yuandong and Ma, Jerry and Gong, Qucheng and Sengupta, Shubho and Chen, Zhuoyuan and Pinkerton, James and Zitnick, C Lawrence},
	date = {2019},
}

@article{tesauro_temporal_1995,
	title = {Temporal difference learning and {TD}-Gammon},
	volume = {38},
	pages = {58--68},
	number = {3},
	journaltitle = {Communications of the {ACM}},
	author = {Tesauro, Gerald},
	date = {1995},
}

@article{le_hierarchical_2018,
	title = {Hierarchical imitation and reinforcement learning},
	journaltitle = {{arXiv} preprint {arXiv}:1803.00590},
	author = {Le, Hoang M and Jiang, Nan and Agarwal, Alekh and Dudík, Miroslav and Yue, Yisong and Daumé {III}, Hal},
	date = {2018},
}

@inproceedings{pirahansiah_effect_2018,
	location = {Kuala Lumpur, Malaysia},
	title = {Effect of Augmented Synthetic Datasets to Train Convolutional neural networks},
	rights = {All rights reserved},
	abstract = {The various number of images and large dataset to train deep neural networks ({DNNs}) are key to accurate model. Dataset with poor translational invariance and lack of information about orientation or more generally what he calls "pose". Pose information refers to 3D orientation relative to the viewer but also lighting and color. Convolutional neural networks ({CNNs}) are known to have trouble when objects are rotated or when lighting conditions are changed. In this paper a method for augmented synthetic dataset ({ASD}) for training {CNNs} is proposed. The utmost issue of utilizing deep learning is its requirement of the large dataset. Moreover, in most of the cases the existence dataset is not large enough to train deep learning properly. In addition, labeling the data is time consuming. In this research image enhancement, transformation and filtering are using for increasing number of images in dataset. Large dataset is required in these methods lead to have more accurate training in {DNN}. After improving the dataset we will compare the accuracy of the new dataset with original one with three different deep learning architecture such as, {LeNet}, {AlexNet}, and {GoogLeNet} by using Caltech-101 and Caltech-256 dataset. Result show that the multi augmentation method can increase the accuracy of {CNNs} by increasing the number of images up to 80 percent in the training phase.},
	booktitle = {2018 2nd Advanced Research in Electronic Engineering and Information Technology International Conference ({AVAREIT}'2018)},
	author = {{PirahanSiah}, Farshid and Hon, Hon Hock Woon and Zulkepeli, Nik Ahmad Akram Nik},
	date = {2018-01},
	keywords = {Convolutional neural networks, Augmented Synthetic Dataset, Computer Vision, Deep Neural Networks, multi augmentation methods, Training {GAN}},
}

@patent{pirahansiah_method_2021,
	title = {A method for detecting a moving vehicle},
	rights = {All rights reserved},
	type = {patent},
	number = {{WO}2021107761A1},
	author = {{PIRAHANSIAH}, Hamam {MOKAYED};Hock Woon Hon;Den Fairol {BIN} {SAMAON};Hasmarina {BINTI} {HASAN};Farshid},
	date = {2021},
}

@patent{hon_method_2021,
	title = {A method for augmenting a plurality of face images},
	rights = {All rights reserved},
	type = {patent},
	number = {{WO}2021060971A1},
	author = {Hon, Farshid {PIRAHANSIAH};Hamam {MOKAYED};Hock Woon},
	date = {2021},
}

@patent{zulkepeli_system_2020,
	title = {System and method for providing advertisement contents based on facial analysis},
	rights = {All rights reserved},
	type = {patent},
	number = {{WO}2020141969A3},
	author = {{ZULKEPELI}, Farshid {PIRAHANSIAH} ; Hock Woon Hon ; Nik Ahmad Akram Bin {NIK}},
	date = {2020},
}